## **Topic 5.10 — Summary**

Chapter 5 ties together all the replication models used in modern distributed data systems and explains **how data is copied, synchronized, and kept consistent across multiple machines** in the presence of failures, latency, and network partitions. The chapter shows that replication is not just a feature—it is a **fundamental architectural decision** that defines the system’s guarantees and limitations.

This summary consolidates the core principles, mechanisms, and trade-offs discussed across the chapter.

---

# **I. Why Replication Matters**

Replication means **maintaining multiple copies of the same data on different nodes**. It serves four key goals:

1. **High Availability**

   * If one node goes down, other nodes still serve requests.

2. **Fault Tolerance**

   * Hardware fails, networks partition—replication prevents data loss.

3. **Latency Reduction**

   * Users read/write to a nearby replica (especially in multi-region systems).

4. **Scalability**

   * Replicas spread read load; multi-leader/leaderless spread write load.

Without replication:

* a single node failure = downtime,
* data is at risk of loss,
* geographic distribution is impossible.

Replication is the **foundation of distributed systems**.

---

# **II. Leader–Follower Replication**

**Leader–Follower (Primary–Replica)** is the simplest and most widely used model.

* **One leader** accepts all writes.
* **Followers** replicate the leader’s log.
* **Reads** can happen from leader or followers.

Flow:

* client writes → leader → replication log → followers.

### **Properties**

* **Strong consistency** on leader reads.
* **Eventual consistency** on follower reads (if async).
* Easy to reason about: one source of truth.

### **Modes**

* **Synchronous replication**

  * leader waits for followers before acknowledging write
  * strong consistency, no data loss in failover
  * slower writes, lower availability.

* **Asynchronous replication**

  * leader acknowledges immediately
  * low latency, high availability
  * risk of losing latest writes on failover.

### **Use Cases**

* Traditional relational databases
* OLTP systems
* Single-region applications

This model provides **serial execution of writes**, making correctness easier.

---

# **III. Handling Failures in Leader–Follower**

Node failures are inevitable.

### **Follower failures**

* easy to handle,
* leader keeps serving writes,
* follower catches up via log replay on restart.

### **Leader failures**

* complex,
* require **failover** (elect new leader),
* risk of **lost writes** if async replication.

Systems need:

* health checks,
* leader election,
* fencing tokens,
* prevention of **split-brain** (two leaders).

Systems like Raft and Paxos were invented to **automate safe leadership change**.

---

# **IV. Replication Logs**

The heart of replication is the **log**.

* **append-only**, ordered sequence of writes,
* ensures followers apply the same operations,
* supports crash recovery.

Two forms:

* **physical logs** (byte-level)
* **logical logs** (operations)

Modern systems provide both:

* WAL for crash safety,
* change log (binlog, oplog) for replication and CDC.

Logs allow:

* efficient catch-up,
* snapshot creation,
* external integration (CDC streams).

---

# **V. Multi-Leader Replication**

Multi-leader replication allows **multiple nodes to accept writes**.

Example:

* users in US write to US node,
* users in EU write to EU node,
* writes replicate between leaders.

### **Strengths**

* improved availability,
* lower latency in geo-distribution,
* offline write support,
* load sharing.

### **Core Problem**

* **conflicting writes**:

  * two leaders update same record differently,
  * must detect and resolve.

Conflict resolution approaches:

* Last Write Wins (LWW)
* version vectors
* application merge logic
* CRDTs

Multi-leader is **eventually consistent** by default.

### **Use Cases**

* collaborative editing,
* mobile apps with sync,
* multi-region active-active deployments.

---

# **VI. Leaderless Replication**

Leaderless replication eliminates the leader entirely.

* any replica can accept reads and writes,
* system uses **quorum voting** for consistency:

  * write quorum (**W**) acknowledgements,
  * read quorum (**R**) responses.

Condition for strong consistency:

> **R + W > N** (number of replicas)

Ensures read and write overlap on at least one node.

### **Mechanisms**

* vector clocks,
* hinted handoff,
* read repair,
* Merkle trees,
* anti-entropy repair.

Leaderless systems offer:

* **high availability**,
* **low latency**,
* **massive write scalability**,
* tolerance to partitions.

But require:

* conflict resolution,
* convergence mechanisms.

### **Use Cases**

* shopping carts,
* key-value storage,
* social actions,
* metrics/log storage.

Inspired by **Amazon Dynamo**.

---

# **VII. Quorums for Reads and Writes**

Quorums control consistency and latency.

Parameters:

* **N** = number of replicas,
* **W** = write quorum,
* **R** = read quorum.

Example:

* N=3, W=2, R=2 → strong consistency.

Tuning **R and W** balances:

* consistency,
* availability,
* latency.

Trade-offs:

* **low W and R** → fast but stale
* **high W and R** → slow but consistent

Choice depends on domain requirements.

---

# **VIII. Trade-offs in Replication**

Replication architectures reflect unavoidable trade-offs:

## **Consistency vs Availability**

* **Leader–Follower (sync)**: consistency prioritized
* **Leaderless (AP)**: availability prioritized
* **Multi-Leader**: availability and latency prioritized, conflicts resolved later

Based on **CAP theorem**:

* during a partition:

  * strong consistency requires rejecting writes,
  * high availability requires accepting writes (leading to eventual consistency).

**PACELC theorem** generalizes:

* even without partition:

  * either low latency or strong consistency.

**Replication is always a trade-off**, not only during failure.

---

# **IX. Choosing the Right Architecture**

Replication model depends on application needs:

### **Leader–Follower:**

* strict consistency
* relative simplicity
* transactional workloads
* single-region systems

Examples:

* banking,
* user auth,
* inventory tracking,
* ticket booking.

---

### **Multi-Leader:**

* geographically distributed writes
* low latency for distant users
* collaborative editing
* offline-first apps

Examples:

* Google Docs,
* mobile sync,
* CRM systems spanning branches.

---

### **Leaderless:**

* high write throughput
* low latency
* merges acceptable
* simple KV data

Examples:

* social likes,
* shopping carts,
* recommendation logs,
* IoT event ingestion.

---

# **X. The Convergence Trend**

Modern databases increasingly combine ideas:

* **consensus per shard** (Raft/Paxos),
* distributed logs,
* followers for reads,
* geo-distribution.

Systems like **Spanner, CockroachDB, TiDB**:

* look like multi-leader globally,
* but avoid conflicts by ensuring **single-leader per key** through consensus.

This achieves:

* global scalability,
* strong consistency,
* no conflict resolution at application level.

It represents the **evolution** of replication models toward **hybrid consensus architectures**.

---

# **XI. Final Key Principles from Chapter 5**

### **1. Replication exists to increase availability, durability, and performance**

Every database must handle hardware failures.

### **2. All replication models rely on logs**

Ordered, append-only logs ensure consistent state application.

### **3. Leader–Follower is simple but limited**

It serializes all writes and requires safe failover.

### **4. Multi-Leader unlocks geo-distributed writes but creates conflicts**

Conflicts are inherent and must be resolved.

### **5. Leaderless systems use quorums and merging**

They scale writes and availability at the cost of eventual consistency.

### **6. Quorum settings are the tuning knobs**

**R**, **W**, and **N** define system guarantees.

### **7. CAP and PACELC govern design choices**

You cannot have everything:

* strong consistency,
* high availability,
* low latency,
* zero data loss.

Design is choosing the right sacrifices.

### **8. Replication is a *business decision***

Not just a technical one:

* how much stale data is acceptable?
* what happens during partitions?
* how fast must writes be?
* how much data loss is tolerable?

---

# **XII. One-Line Summary**

> **Replication allows a distributed system to survive failures and scale, but every replication model forces trade-offs between consistency, availability, latency, throughput, and operational complexity.**

The right model depends entirely on **application requirements**, not technical ideology.