## **Topic 5.9 — Trade-offs in Replication**

Replication is not a single feature—it represents a set of design choices that determine how a distributed database behaves under normal conditions and during failures. Every replication architecture involves inherent **trade-offs** between **consistency, availability, latency, throughput, operational complexity, and fault tolerance**.

This topic ties together all replication models covered in Chapter 5 (leader–follower, multi-leader, leaderless), analyzes their strengths and weaknesses, and explains why real-world systems must choose one replication model based on business needs rather than seeking a “perfect” solution.

Replication is where the **CAP theorem**, eventual consistency, quorum strategies, consensus protocols, and conflict resolution all intersect.

---

# **I. Why Trade-offs Exist**

Distributed systems face fundamental limitations:

1. **Network is unreliable**

   * delays, drops, partitions
2. **Nodes can fail arbitrarily**

   * crashes, power loss
3. **Clocks are not perfectly synchronized**

   * timestamps can’t be trusted fully
4. **Latency is unpredictable**

   * geographic distance matters
5. **Impossible to guarantee both availability and strict consistency during partitions**

   * (CAP theorem)

So replication strategies must prioritize different failure scenarios and application requirements.

There is **no free lunch**:

* stronger consistency means lower availability,
* faster writes mean weaker durability,
* simpler architecture means less flexibility.

---

# **II. Summary of Replication Models**

Let’s compare the three major replication architectures:

## **1. Leader–Follower**

* One leader processes all writes
* Followers replicate changes
* Reads can scale horizontally
* Synchronous or asynchronous replication
* Failover possible but complex

Best for:

* strong consistency,
* predictable conflict behavior,
* traditional relational workloads,
* OLTP systems.

Trade-offs:

* single write bottleneck,
* failover risk,
* latency for remote clients.

---

## **2. Multi-Leader**

* Multiple leaders accept writes
* Leaders replicate to each other
* Avoids single write bottleneck
* Good for geo-distribution
* Complex conflict resolution

Best for:

* multi-region active-active apps,
* mobile/offline sync apps,
* collaborative editing systems.

Trade-offs:

* conflicts are inevitable,
* complex merge logic,
* eventual consistency across leaders.

---

## **3. Leaderless (Quorum-Based)**

* No fixed leader
* Writes/read to any replica
* Quorums ensure convergence
* Highly available under partitions

Best for:

* high-throughput KV stores,
* latency-sensitive global systems,
* applications tolerant of merging,
* AP workloads.

Trade-offs:

* eventual consistency by design,
* client-side conflict resolution,
* complex versioning metadata.

---

# **III. Dimensions of Replication Trade-offs**

Replication choices affect these dimensions:

---

## **1. Consistency**

### **Strong consistency**

* All clients see latest value
* Reads block until state is updated
* Requires:

  * synchronous replication
  * Quorum R + W > N
  * leader-based serial log

Good for:

* transactional systems,
* financial operations,
* integrity-critical data.

---

### **Eventual consistency**

* Replicas converge eventually
* Reads may see stale data
* Offers:

  * lower latency,
  * higher availability,
  * tolerance to partitions.

Good for:

* user-generated content,
* social feeds,
* shopping carts,
* metrics/analytics.

---

**Choosing consistency is a trade-off against availability and latency.**

---

## **2. Latency**

Replication introduces latency:

* synchronous = slower writes (must wait for followers)
* asynchronous = fast writes, stale reads
* cross-region = slow, highly variable latency
* quorum = latency = slowest replica in quorum

If latency matters (e.g. mobile users), choose:

* local writes,
* asynchronous replication,
* multi-leader architecture.

If correctness matters:

* accept latency cost,
* use synchronous or quorum.

---

## **3. Availability**

Availability means **systems continue to accept operations during failures**.

During a network partition:

* **AP systems** accept operations:

  * leaderless,
  * multi-leader,
  * async leader–follower.

* **CP systems** reject operations:

  * synchronous leader–follower,
  * consensus-based systems.

Applications choose:

* Do we allow stale writes to keep system online?
* Or reject writes to preserve consistency?

This is a business decision, not technical.

---

## **4. Durability / Data Loss Risk**

Asynchronous replication risks data loss if leader crashes before followers receive writes.

* Synchronous replication = lower loss risk
* Asynchronous replication = possible loss
* Majority quorum = safe commit

Trade-off:

* strong durability = higher latency
* low latency = weaker durability

Applications with strict durability needs:

* finance,
* banking,
* critical invoices,

Should choose:

* consensus/quorum,
* synchronous replication.

---

## **5. Write Throughput**

Leader–follower limits write throughput to one leader:

* bottleneck
* must scale vertically

Multi-leader and leaderless spread writes:

* multiple nodes accept writes
* horizontal scaling
* better throughput

Trade-off:

* more nodes = conflict complexity
* writes become distributed system problem

---

## **6. Read Scalability**

All architectures support read scaling, but differently:

Leader–Follower:

* simple follower-based read scaling
* control staleness with client config

Multi-Leader:

* each node serves reads
* eventual consistency between leaders

Leaderless:

* all nodes serve reads
* quorum or eventual consistency

Trade-off:

* reading latest vs reading fastest

---

## **7. Operational Complexity**

Several trade-offs exist in operations:

### **Leader–Follower**

* simple mental model
* complex failover
* monitoring lag
* load balancing reads

### **Multi-Leader**

* conflict detection/resolution
* version vectors
* merging logic
* circular replication bugs

### **Leaderless**

* quorums require careful tuning
* versioning metadata
* hinted handoff
* anti-entropy repair
* read repair overhead

Some systems are easier to **operate**, others are easier to **reason about**, but rarely both.

---

# **IV. Conflict Resolution Trade-offs**

### **Avoid conflicts vs handle conflicts**

Choices:

* Central leader avoids conflicts,
  but bottlenecks writes.

* Multi-leader allows conflicts,
  but requires resolving them.

* Leaderless requires merging logic,
  even at client.

Which is preferable?

Depends on the domain.

Example:

* For **banking transactions**, you must avoid conflict.
* For **shopping cart**, you can merge items.

Conflict resolution strategies:

* Last-write-wins (LWW)
* version vectors
* CRDTs
* custom domain logic

---

# **V. CAP Theorem Implications**

CAP states that during a network partition, a distributed system must choose between:

* **Consistency (C)**

  * all clients see the same data
* **Availability (A)**

  * the system continues accepting reads/writes
* **Partition Tolerance (P)**

  * system functions across network failures (required)

You **cannot** have both strong consistency and full availability under partitions.

So replication architecture inherently chooses:

### **CP systems**

* Leader-based with synchronous replication
* Consensus systems (Raft, Paxos)
* Google Spanner, etcd

Reject writes during partitions, preserve consistency.

### **AP systems**

* Asynchronous replication
* Multi-leader
* Leaderless (Dynamo, Cassandra)

Allow writes during partitions, converge later.

---

# **VI. PACELC Theorem**

While CAP focuses on **behavior during partition**, PACELC extends it to **behavior without partition**:

> **If Partition (P): choose Availability (A) or Consistency (C); Else (E): choose Latency (L) or Consistency (C).**

So systems also trade off latency vs consistency even without failures.

Example:

* synchronous replication → high latency & strong consistency
* async replication → low latency & eventual consistency

Thus, trade-offs exist **always**, not only during partitions.

---

# **VII. Geo-Distribution Trade-offs**

Geo-distributed applications must decide:

* Where are users located?
* Where do writes occur?
* What latency is acceptable?
* What consistency is required?

Architectures:

### **Single leader in one region**

* simple
* high latency for remote users

### **Multi-leader**

* low local latency
* conflict merging required

### **Leaderless quorum**

* consistent reads if quorum local
* heavy coordination cost if global

### **Consensus per-shard**

* each shard elects local leader
* global writes routed to shard leader

Used in:

* CockroachDB
* Spanner

---

# **VIII. Schema Changes and Evolution**

Replication complicates schema changes:

* leader–follower:

  * apply to leader, replicate change
  * ensure compatibility

* multi-leader:

  * schema must be **backward-compatible** everywhere
  * complex schema migration coordination

* leaderless:

  * client must handle multiple schema versions concurrently
  * merging logic must consider schema differences

Schema evolution is a **major operational cost** in multi-source replication systems.

---

# **IX. Idempotency, Retry, and Delivery Guarantees**

Distributed systems must handle:

* duplicate writes,
* retries,
* at-least-once delivery,
* at-most-once delivery,
* exactly-once semantics (very hard)

Architectural choices affect **fault tolerance guarantees**:

* synchronous replication → fewer retries,
* async replication → more retries,
* quorum → duplicate writes must be idempotent.

Designs must avoid:

* double-charging,
* duplicate notifications.

---

# **X. When to Choose Which Architecture**

Decision depends on domain:

### **Choose Leader–Follower when:**

* strong consistency required,
* conflict resolution not acceptable,
* OLTP transactions,
* complex relational data,
* foreign keys, unique constraints.

Examples:

* banking systems
* user accounts
* critical configuration data

---

### **Choose Multi-Leader when:**

* geo-distributed writes,
* offline editing,
* collaborative documents,
* mobile sync,
* merges acceptable.

Examples:

* Google Docs,
* mobile apps syncing user settings,
* distributed content creation.

---

### **Choose Leaderless when:**

* high write throughput,
* low latency,
* eventually consistent workloads,
* mergeable updates,
* simple key-value operations.

Examples:

* shopping cart,
* product catalogs,
* social graph likes/follows,
* metrics storage.

---

# **XI. Consensus vs Quorums vs Conflict Resolution**

Modern systems often combine ideas:

* every partition or range uses **Raft** (consensus)
* globally, system appears **multi-leader**
* clients write to nearest node
* log replication ensures consistency per shard
* conflict avoided by per-key leadership

This hybrid model:

* removes conflict via consensus,
* enables geo-distributed writes,
* preserves strong consistency,
* avoids multi-leader problems.

Used in:

* Spanner,
* CockroachDB,
* TiDB.

---

# **XII. Summary Table of Trade-offs**

### **Replication Architecture Trade-offs**

| Model           | Write Scalability | Consistency | Availability | Conflict | Latency |
| --------------- | ----------------- | ----------- | ------------ | -------- | ------- |
| Leader–Follower | Low               | Strong      | Medium       | None     | Medium  |
| Multi-Leader    | Medium            | Eventual    | High         | Yes      | Low     |
| Leaderless      | High              | Tunable     | High         | Yes      | Low     |

### **Consistency vs Availability**

| Config       | Guarantee | Availability |
| ------------ | --------- | ------------ |
| Sync leader  | Strong    | Low          |
| Async leader | Eventual  | High         |
| Quorum R+W>N | Strong    | Medium       |
| R=1, W=1     | Eventual  | Very high    |

### **Durability risk**

| Architecture | Data Loss Risk |
| ------------ | -------------- |
| Sync leader  | Very low       |
| Async leader | Moderate       |
| Multi-leader | High           |
| Leaderless   | Depends on W   |

---

# **XIII. Key Takeaway Principle**

> **Replication requires choosing trade-offs between consistency, availability, latency, throughput, durability, and operational complexity. Each replication architecture — leader–follower, multi-leader, and leaderless — embodies a different set of trade-offs aligned with specific application needs. There is no universally “best” architecture.**

The correct replication model should be driven by:

* business requirements,
* tolerance for stale data,
* merge complexity,
* failure scenarios,
* latency sensitivity,
* geographic distribution.

Distributed systems are about **managing trade-offs**, not eliminating them.