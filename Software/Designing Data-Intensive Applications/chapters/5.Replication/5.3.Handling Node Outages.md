## **Topic 5.3 — Handling Node Outages**

In a replicated database, it’s not a question of *if* nodes will fail—but *when*. Servers can crash, networks partition, disks fail, or power goes out. Designing replication systems means defining how the system behaves under failure conditions. The challenge is **maintaining consistency, availability, and durability** even when parts of the system are broken.

This section explores **how leader–follower systems handle node outages**, including the behaviors during failure, the recovery steps after failover, and the technical strategies that ensure correctness in the presence of faults.

---

# **I. Types of Node Outages**

Common outage scenarios:

1. **Follower failure**
2. **Leader failure**
3. **Network partition (split-brain)**
4. **Slow or lagging nodes**
5. **Transient outages**
6. **Permanent hardware failures**

Each type requires different handling strategies.

---

# **II. Follower Failure (Simple Case)**

Follower outages are the easiest to handle because they **do not affect write availability**:

### **What happens when a follower fails?**

* Leader continues accepting writes
* Writes are logged normally
* Follower simply stops replicating

### **When follower comes back:**

* It needs to **catch up** to the leader:

  * Replay missed log entries
  * Or load a snapshot and resume

This process is called **recovery** or **resync**.

---

## **Recovering Followers**

Two main approaches:

### **1. Log Replay**

If the replication log is retained:

* Follower reads log entries since its last known position
* Applies them in sequence
* Catches up

This assumes **enough log retention**.

---

### **2. Full Snapshot + Incremental Catch-up**

If follower lag is too large:

* Use a fresh snapshot from the leader
* Apply remaining log entries after snapshot

This prevents:

* replaying years of logs
* slow recovery

---

## **Follower Recovery Challenges**

* **Disk differences** may cause checksum errors
* **Schema changes** may prevent replay
* **Memory pressure** during rebuilds
* **Log truncation** may require snapshot

Robust recovery systems automate these complexities.

---

# **III. Leader Failure (Complex Case)**

Leader outages are more complex because **writes cannot be accepted without a functional leader**. Recovery steps:

### **1. Detect Leader Failure**

Systems use:

* Heartbeats
* Timeouts
* Health checks

to detect nonresponsive leaders.

Failure detection is tricky:

* must avoid false positives,
* network latency ≠ failure,
* requires careful timeout configuration.

---

### **2. Elect a New Leader**

Upon detecting failure:

* Cluster chooses a follower to promote
* Promotion must ensure **data consistency**

Promotion algorithms include:

* Raft consensus
* Paxos
* Zookeeper-based election
* Manual failover (in early systems)

---

### **3. Redirect Clients to New Leader**

Clients discover the new leader using:

* DNS remapping
* Cluster metadata configuration
* A coordinator or consensus service

Failover should be **automatic** in modern deployments.

---

# **IV. Consistency Concerns During Failover**

A critical concern:

> Was the failed leader’s last acknowledged write replicated?

## **Possibility of Data Loss**

### **If replication is asynchronous:**

* Leader may have acknowledged writes **not yet replicated**
* New leader does not have those writes
* Those writes will be **lost**

This creates a **rollback** situation.

Example:

* user sees “Success” on write
* leader crashes
* new leader does not have that data
* user data disappears

This is called **"failover-induced data loss"**.

---

### **If replication is synchronous:**

* Followers have the write
* Safe to promote follower
* No data loss

This is a key trade-off between availability and durability.

---

## **Problem: Two Leaders (Split-Brain)**

What if the old leader isn’t really dead?

Example scenario:

* network partition isolates leader from followers
* followers elect a new leader
* old leader recovers
* now two leaders are accepting writes

This is catastrophically unsafe because their states diverge.

Mitigation requires:

* **leases**
* **“fencing” tokens**
* **quorum-based decision making**
* **consensus algorithms**

Without consensus, split-brain can corrupt a database.

---

# **V. Handling Slow or Lagging Followers**

Followers may fall behind in processing logs due to:

* disk performance issues
* network congestion
* heavy reads
* CPU saturation

Performance impact:

* followers return **stale reads**
* metrics: “replication lag”

Mitigation:

* metrics monitoring
* throttling leader writes
* load balancing reads to up-to-date nodes
* dynamic follower promotion logic

---

# **VI. Recovery After Outage**

Once failed nodes come back, they must rejoin the cluster safely.

### **Follower Rejoining**

Process:

1. Validate its previous log offset
2. Check log availability
3. Catch up via log or snapshot
4. Resume normal replication

### **Leader Rejoining**

Instead of returning as leader:

* previous leader returns as follower
* must discard any uncommitted log entries
* catch up from new leader
* prevents conflict

This requires:

* ability to **roll back trailing state**
* compare log positions
* consensus coordination

---

# **VII. Practical Techniques for Safe Failover**

### **1. Write-Ahead Log Matching**

Compare log entries to find last committed entry.

### **2. Epoch or Term Numbers**

Each leadership period gets a number (Raft calls this **term**).

* followers reject writes from old leaders
* prevents outdated leaders from corrupting state

### **3. Quorum Writes**

Writes accepted only if majority replicate them.

* protects against data loss
* makes election safe

Used in:

* Raft
* Spanner
* Cassandra (quorum mode)

---

# **VIII. System Behavior During Outages**

Behavior depends on replication mode:

| Failure         | Sync               | Async                |
| --------------- | ------------------ | -------------------- |
| Follower outage | Writes continue    | Writes continue      |
| Leader outage   | Safe failover      | Possible data loss   |
| Partition       | System blocks      | System diverges      |
| Read behavior   | Strong consistency | Eventual consistency |
| Promotion       | Automatic          | Risky                |

This is closely tied to CAP theorem: **Consistency vs Availability**.

---

# **IX. Operational Concerns**

Real systems must implement:

* monitoring for leader health
* metrics such as:

  * replication lag
  * number of queued logs
  * follower catch-up speed
* alerts on:

  * long failovers
  * frequent elections
  * unstable leadership

Operations can be more complex than design.

---

# **X. User Experience and Failover Transparency**

Ideal behavior:

* failover takes a few seconds
* applications retry automatically
* no human intervention

Tools involved:

* load balancers
* client retry logic
* idempotent writes
* exponential backoff

Modern cloud platforms automate failover behavior.

---

# **XI. Key Principle**

> **Follower outages are easy to handle because writes continue; leader outages are hard because failover must preserve consistency and avoid data loss. Designing safe failover requires consensus, log management, and clear leadership discipline.**

Successful systems make failover:

* **automatic**
* **safe (no split-brain)**
* **predictable**
* **transparent to clients**
