## **Topic 5.4 — Setting Up New Followers**

When a follower crashes, a new node is added, or a replica is scaled out, the system must **initialize a new follower** so it contains a **consistent copy** of the leader’s data. This process is more complex than simply copying files: the follower must catch up with ongoing writes while the snapshot is being created, ensure consistency, and avoid disrupting the leader’s operations.

This section explains how systems set up new followers, how they synchronize data, and what challenges arise in doing so.

---

# **I. Why Setting Up New Followers Is Nontrivial**

Unlike static backups:

* The leader **continues receiving writes** during follower setup.
* Data is constantly changing, so a snapshot alone is **not enough**.
* A naive file copy may produce **inconsistent state**.

Therefore, the process must combine:

1. **Snapshot (base image)** → initial state, **plus**
2. **Log replay (change events)** → updates after the snapshot

This ensures the new follower catches up to “present time” while the system continues serving traffic.

---

# **II. Steps to Initialize a New Follower**

Typical initialization process:

### **Step 1 — Create a Snapshot**

* Leader creates a consistent snapshot of its current data.
* Snapshot reflects a moment-in-time view.
* Snapshot is copied to the new follower.

Snapshot methods:

* **File copy while database is locked** (rare)
* **Snapshot without stopping writes** (copy-on-write)
* **Physical backup tools** (e.g., `pg_basebackup` in PostgreSQL)
* **Storage-level snapshot** (EBS snapshots, LVM)
* **Logical dump** (`mysqldump`, `pg_dump`) — slower

Snapshots must include:

* tables,
* indexes,
* metadata,
* and potentially WAL/log positions.

### **Step 2 — Transfer Snapshot**

Snapshots may be large (GBs–TBs). Transfer must be efficient and reliable.

* Compression
* Parallel streaming
* Incremental syncing
* Hardware-assisted storage replication

### **Step 3 — Tail the Log from Snapshot Position**

Once snapshot is installed, the follower:

* **replays log entries** generated since the snapshot time.
* Applies writes in the same order as leader.

This step is crucial:

* It reconciles the initial snapshot with live writes.
* The follower eventually reaches the same state as leader.

### **Step 4 — Transition to Live Replication**

Once caught up:

* follower enters normal replication mode
* receives new writes in real-time

Follower now participates in the cluster.

---

# **III. Consistency Challenges**

The main challenge is ensuring the new follower’s state is **exactly identical** to the leader’s, despite ongoing writes.

Key considerations:

### **1. Snapshot Isolation**

Snapshot must be consistent at a specific log position.

* Without isolation → FOLLOWER may see partial writes,
* leading to different state.

Techniques include:

* MVCC
* copy-on-write (COW)
* snapshot isolation transaction
* filesystem-level snapshots

### **2. Log Position Tracking**

Leader must record **the exact log offset** during snapshot creation.

Follower must apply log entries from that precise offset.

If offsets mismatch:

* follower will drift,
* causing fatal state divergence.

---

# **IV. Why Not Just Copy Files?**

Example failure case:

* Snapshot taken at 12:00:00
* Leader processes writes A, B, C at 12:00:01
* Follower installs snapshot at 12:00:10 without those writes
* Without log replay, follower is missing data (A, B, C)

If follower then receives new writes, its state will be **inconsistent** with leader.

Hence the **log replay** stage is required.

---

# **V. How Databases Implement Snapshot + Log Replay**

Different systems implement different mechanisms.

### **PostgreSQL**

* Uses **WAL (Write-Ahead Log)** for snapshot replay
* `pg_basebackup` + WAL streaming

### **MySQL**

* Uses **binary logs** (binlogs)
* `mysqldump` + binlog position
* Or `hot backup` modes

### **MongoDB**

* Oplog tailing
* Snapshot + oplog replay

### **Cassandra**

* SSTable snapshot + commit log replay

### **HBase**

* HDFS snapshots + write-ahead logs

The core idea is the same across systems.

---

# **VI. Catch-Up Performance**

Replaying logs can be slow if:

* large backlog,
* slow disk,
* slow network,
* large number of small writes.

Strategies to speed up catch-up:

* Rate limiting leader writes (rare)
* Increasing follower parallelism
* Using storage snapshots instead of logical dumps
* Using incremental snapshots
* Adding more hardware resources temporarily

Production systems often automate catch-up for ease of scaling.

---

# **VII. Snapshot Sources**

Snapshots can come from:

### **1. Primary Node**

Leader itself generates snapshot.

Pros:

* simple

Cons:

* heavy load on leader
* slower under heavy write traffic

### **2. Existing Follower**

Follower creates snapshot and transfers it.

Pros:

* leader unaffected
* offloads work

Cons:

* follower must be consistent and up-to-date

Large clusters use **cascading replication**:

* followers replicate from other followers
* reduces load on leader

---

# **VIII. Cascading Replication**

Instead of every follower replicating directly from the leader:

```
Leader
 ├─ Follower A
 │    ├─ Follower A1
 │    └─ Follower A2
 └─ Follower B
      ├─ B1
      └─ B2
```

Benefits:

* leader load reduced
* hierarchical replication
* improved scalability
* smaller latency chain for geographically distributed clusters

Drawbacks:

* more complex failure handling
* if middle-tier node fails, deeper tiers must reconfigure

---

# **IX. Rejoining Failured Nodes Safely**

When a failed follower restarts, it must:

1. discard any incomplete writes
2. verify log offset with leader
3. replay logs
4. resume normal operation

If a follower has divergent writes that didn’t come from leader (should never happen in leader–follower), they must be purged.

Thus:

* **followers are not allowed to accept writes**
* write paths are only through leader

This prevents data divergence.

---

# **X. Snapshot Consistency Levels**

Snapshots may be:

* **consistent point-in-time**
* **crash-consistent**
* **application-consistent**

For reliable recovery:

* application-consistent snapshots matter:

  * all in-flight transactions resolved,
  * no partial writes,
  * indexes stable

Crash-consistent snapshots may need recovery on follower startup.

---

# **XI. Distributed Systems and Snapshot Evolution**

Modern systems like CockroachDB use:

* distributed snapshots + Raft logs per shard
* atomic snapshots per range
* fast catch-up via log truncation control

In cloud architectures:

* **region-level replicas** may use asynchronous cross-region snapshots
* **zone-level replicas** may use synchronous snapshot and consensus

This allows:

* local consistency,
* global low-latency reads.

---

# **XII. Key Principle**

> **Initializing a new follower requires a consistent snapshot of the leader’s data plus replaying the leader’s replication log to catch up on writes that occurred during the snapshot process.**

This ensures:

* consistency with the leader,
* ability to join the cluster without downtime,
* safe read scaling,
* and predictable failover behavior.

The process must be:

* automated,
* efficient at scale,
* tolerant to ongoing writes,
* safe against partial or corrupted state.