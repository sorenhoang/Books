## **Topic 5.8 — Quorums for Reads and Writes**

In leaderless replication systems, writes and reads must be coordinated so that all replicas eventually **converge to the same state**, and so that reads return the **most recent value** (with some defined guarantee). The typical mechanism to coordinate reads and writes in a distributed, leaderless environment is the **quorum mechanism**.

A **quorum** is a subset of replicas whose votes (acknowledgements) are sufficient to confirm that an operation is successful. By choosing quorum sizes appropriately, the system guarantees that reads and writes **overlap**, ensuring at least one replica in the read quorum has the most recent committed write.

This section explains quorum math, how quorums guarantee consistency, and what trade-offs exist across different quorum configurations.

---

# **I. Replication Factor: N**

To understand quorum behavior, we start with replication factor:

```
N = total number of replicas that store a piece of data
```

Examples:

* N = 3 (common),
* N = 5 (higher safety),
* N = 1 (no replication).

A higher N:

* increases durability,
* increases fault tolerance,
* increases storage cost,
* increases write latency.

Leaderless replication typically operates with **N ≥ 3**.

---

# **II. Write Quorum: W**

```
W = number of replicas that must acknowledge a write
```

For a write to be considered **committed**, the coordinator (client, proxy node, or coordinator node) must receive at least **W acknowledgements**.

The write path is:

1. Client sends write request
2. Coordinator sends write to all N replicas (or some subset)
3. Each replica stores the write (locally durable)
4. Replicas acknowledge to coordinator
5. Once W acknowledgements received → write committed

**W controls durability and consistency of writes**

* larger W = stronger guarantees
* smaller W = faster writes

---

# **III. Read Quorum: R**

```
R = number of replicas that must return a value for a read
```

For a read request to be considered complete, the coordinator must receive at least **R responses**.

The read path is:

1. Client sends read request
2. Coordinator queries R replicas (or all N, but waits for R)
3. Receives versioned values
4. Chooses correct value (resolve conflicts if needed)
5. May perform **read repair**
6. Returns result to client

**R controls staleness and latency of reads**

* larger R = higher chance of reading latest data
* smaller R = faster reads

---

# **IV. Ensuring Strong Consistency: R + W > N**

The core guarantee of quorum reads/writes:

> **If R + W > N, then read and write quorums overlap, guaranteeing that at least one replica used for the read has the latest committed write.**

Example:

* N = 3
* W = 2
* R = 2

Then → R + W = 4 > 3 → **overlap = 1 replica**

Meaning:

* every write must reach at least 2 replicas
* every read must consult at least 2 replicas
* so every read sees at least one replica that was written

Thus the client **never reads stale data** if there are no failures.

This provides **linearizable consistency** for a single key, if synchronous durability is ensured.

---

# **V. Quorum Configurations and Their Guarantees**

Different quorum configurations yield different consistency, durability, and latency trade-offs.

## **Case 1: Strong Consistency**

```
R + W > N
```

Guarantees:

* latest committed write visible
* no stale reads
* writes visible immediately

Cost:

* slower reads/writes (because waiting for quorum)
* lower availability

---

## **Case 2: Eventual Consistency**

```
R + W <= N
```

Example:

* N = 3, W = 1, R = 1
* R + W = 2 ≤ 3 → no overlap guaranteed
* write may only reach 1 replica
* read may hit stale nodes

Advantages:

* super fast,
* maximum availability.

But:

* data may be stale,
* or conflicting,
* conflict resolution needed.

---

## **Case 3: Write-Heavy Tuning**

```
W = majority, R = 1
```

Example:

* N = 3: W = 2, R = 1

Guarantees:

* writes require majority,
* reads are fast
* read always sees latest value if failures < N/2

Works well if:

* writes are frequent,
* reads need low latency.

---

## **Case 4: Read-Heavy Tuning**

```
R = majority, W = 1
```

Example:

* N = 3: W = 1, R = 2

Guarantees:

* reads require majority,
* writes are fast.

Works well if:

* reads are majority,
* writes are low frequency.

---

# **VI. Latency and Slow Nodes**

Latency of quorum operations is controlled by the **slowest node in the quorum**.

For a write:

* latency = time to get W acknowledgements

For a read:

* latency = time to get R responses

If a replica is slow:

* coordinator may use fastest replicas,
* but maintain quorum.

If **R or W** is too large:

* a single slow replica slows the operation.

Thus, large quorums ensure safety but compromise latency.

---

# **VII. Quorums Under Failures**

Suppose N = 3, W = 2, R = 2.

**If 1 node fails:**

* write quorum still possible (2 available),
* read quorum still possible (2 available).

**If 2 nodes fail:**

* write quorum impossible,
* read quorum impossible,
* system must reject operations,
* ensures safety over availability.

This is a **CP system** (Consistent + Partition-tolerant).

If **W = 1, R = 1**:

* even if 2 nodes fail, 1 node can handle operations
* this risks consistency but maximizes availability (**AP system**).

---

# **VIII. Coordination Variants**

Leaderless systems can implement quorums in different ways:

## **1. Client-side Coordinators**

Client:

* sends to N replicas
* waits for W acknowledgements
* collects R values
* picks correct value

This requires client logic for:

* versioning,
* conflict resolution,
* retry.

Used in the original **Amazon Dynamo** design.

---

## **2. Coordinator Node**

A node acts as temporary coordinator:

* client sends request to coordinator,
* coordinator handles quorum logic,
* returns final value.

Cassandra and Riak use coordinator nodes to reduce client complexity.

---

## **3. Dynamo-style Protocol**

Dynamo uses:

* **vector clocks** for versioning,
* **read repair** on reads,
* **hinted handoff** for failed replicas.

It tracks **context** of values to detect concurrent writes.

---

# **IX. Read Repair and Data Convergence**

When reading, the coordinator:

* compares R values,
* chooses the latest value,
* asynchronously updates stale replicas.

This is **read repair**, ensuring replicas converge over time.

If no reads happen for a key:

* background repair process uses **Merkle trees** (hash trees) to detect differences between replicas,
* syncs them efficiently.

Thus, consistency is **gradually restored** without global locks.

---

# **X. Hinted Handoff to Handle Failures**

If a replica is down during write:

* another replica stores the write **on behalf of** the failed replica,
* stores a “hint” (timestamped),
* delivers it when node returns.

This avoids **write loss** during partial failures.

Example:

* N = 3,
* node C fails,
* node A writes on behalf of C,
* when C recovers, A forwards missed writes.

This preserves durability and avoids write rejection.

---

# **XI. Sloppy Quorums**

Some systems allow **sloppy quorums**:

* in presence of failure, coordinator uses **any available nodes**, not strictly the N replicas assigned for that key.

Example:

* N = 3, W = 2,
* one primary replica is down,
* commit to:

  * replica1,
  * replicaX (different owner),
* later:

  * hinted handoff routes writes back to original replica.

This increases **availability** at expense of consistency.

---

# **XII. Quorum Math Examples**

### **Example 1: Strong Consistency**

```
N = 3
W = 2
R = 2
R + W = 4 > 3
```

Result:

* strong consistency
* moderate latency
* tolerates 1 failure

---

### **Example 2: Eventual Consistency**

```
N = 3
W = 1
R = 1
R + W = 2 ≤ 3
```

Result:

* extremely fast
* high availability
* stale reads possible

---

### **Example 3: Balanced**

```
N = 5
W = 3
R = 2
R + W = 5 > 5? No (== 5 OK but border)
```

To ensure strict overlap R + W must be ≥ N+1.
So:

* R = 3, W = 3 (R + W = 6 > 5)
* ensures 1 overlap

But costs:

* very high latency,
* rejects writes if >2 failures.

Thus, N = 5 often uses:

* R = 2, W = 3 (overlap = 1)

---

# **XIII. CAP and Quorums**

Quorums allow leaderless systems to implement **tunable consistency**:

* want consistency? increase W and R
* want availability? reduce W and R

But cannot violate CAP:

* during partition, either accept stale data or reject writes

Quorums allow **graceful degradation**.

---

# **XIV. Comparison With Consensus**

Consensus protocols (e.g., Raft, Paxos):

* require **majority** for every write
* enforce **global total order**
* maintain one leader per shard
* reject operations without quorum

Leaderless quorums:

* allow writes with partial quorums
* allow conflicts
* converge eventually
* client has to handle resolution

Consensus is **CP**,
Leaderless is **AP with tunable C**.

---

# **XV. When to Use Quorum-Based Leaderless Replication**

**Appropriate for:**

* high-throughput key-value workloads
* geographically distributed clusters
* applications tolerant of eventual consistency
* systems needing offline writes
* use cases with mergeable updates (e.g., counters, sets)

**Not appropriate for:**

* strict ACID transactions
* complex constraint enforcement
* cross-record consistency
* financial operations

---

# **XVI. Key Takeaway Principle**

> **Quorums ensure that read and write operations overlap in enough replicas to guarantee that at least one replica in a read quorum has the latest committed write. The condition R + W > N guarantees consistency for a single key. Quorum size directly controls the trade-off between consistency, availability, and latency.**

Quorum-based replication:

* eliminates leader,
* distributes writes,
* increases failure tolerance,
* but requires careful configuration of R and W.