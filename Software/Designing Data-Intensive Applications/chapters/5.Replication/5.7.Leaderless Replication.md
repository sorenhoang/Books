## **Topic 5.7 — Leaderless Replication**

Leaderless replication is a replication model where **no single node is designated as the leader** for write operations. Instead, **clients send writes to multiple replicas directly**, and the system uses **quorum-based protocols** to ensure consistency. This architecture powers many large-scale, distributed NoSQL databases, such as Amazon Dynamo, Cassandra, Riak, and Voldemort.

The key idea is that **all replicas are equal**: any replica can receive reads or writes, and the system uses **coordinated voting/quorums** to determine which values are valid.

This model provides **high availability, fault tolerance, and partition tolerance**, especially in distributed environments where network partitions are common.

---

# **I. Why Leaderless Replication Exists**

The leader–follower model has limitations:

* Single leader becomes a **bottleneck** for writes.
* Leader failure triggers complicated **failover**.
* Remote clients experience **high latency**.
* Leader saturation causes **traffic spikes**.

Leaderless replication solves these issues by:

* distributing write load,
* eliminating failover complexity,
* allowing writes during network partitions,
* enabling high write throughput via parallelism.

This model favors **availability over strict consistency** and aligns with the **AP side of the CAP theorem** (Available + Partition-tolerant).

---

# **II. How Leaderless Replication Works**

Assume the database stores **N replicas** for each piece of data.

When a client writes:

1. Client sends write to **multiple replicas**
2. Replicas acknowledge write
3. Once **enough** replicas respond (quorum), operation is considered successful

When a client reads:

1. Client sends request to multiple replicas
2. Receives multiple versions
3. **Client-side or coordinator-side resolution** returns correct value

There is no single leader node managing all writes.

---

# **III. Quorum-Based Replication**

Leaderless systems rely on quorums.

Let:

* **N** = number of replicas (e.g. 3)
* **W** = number of replicas that must acknowledge a write
* **R** = number of replicas that must respond to a read

To ensure strong consistency:

> **R + W > N**

Meaning read and write quorums overlap in at least one node.

Example:

* N = 3
* W = 2
* R = 2

Overlap = 1 replica guaranteed to have the latest write.

Thus, a read that hits 2 nodes will see the correct value if 2 writes succeeded.

This is called a **quorum configuration**.

---

## **Different Consistency Levels**

* **W = 1**: high availability, risk of stale reads
* **R = 1**: low latency, fragile consistency
* **R + W > N**: strong consistency
* **R + W ≤ N**: eventual consistency

Applications choose **R and W** based on their tolerance for:

* stale reads,
* write latency,
* data safety,
* failure handling.

---

# **IV. Handling Node Failures**

Leaderless replication tolerates failures naturally.

Example:

* N = 3 replicas
* W = 2 required
* If one node fails:

  * remaining 2 nodes form quorum
  * writes continue

No special election process required.

When failed node recovers:

* it receives **read-repair** or **hinted-handoff** operations
* data synchronizes gradually

This results in:

* better write availability,
* simpler failure recovery.

---

# **V. Versioning and Conflict Resolution**

Because multiple replicas accept writes simultaneously, the system must reconcile divergent values.

Several mechanisms ensure convergence:

## **1. Last Write Wins (LWW)**

* each write carries a timestamp
* latest timestamp wins

Simple, but may lose concurrent updates.

---

## **2. Vector Clocks**

Track causality across replicas:

* every replica has version metadata
* store multiple versions if conflict
* resolve later or expose conflict to client

Vector clocks detect:

* concurrency,
* overwrites,
* missed updates.

Used in:

* Amazon Dynamo
* Riak

---

## **3. Read Repair**

During reads:

* client receives multiple versions
* compares them
* writes back correct (latest) value to stale replicas

Ensures eventual convergence.

Example:

* read sees that replica2 is old
* client updates replica2 asynchronously

This is **client-driven repair**.

---

## **4. Hinted Handoff**

When a node is down:

* another node stores a “hint” (pending write)
* when node returns, hinted writes are delivered

Prevents write loss during partial outages.

---

## **5. Anti-Entropy Repair**

Background process:

* nodes compare hash trees (Merkle trees) of data
* synchronize differences

Like rsync for data divergence.

Used in:

* Cassandra (Anti-Entropy Repair)
* Riak

Key benefit:

* low network overhead for detecting inconsistency

---

# **VI. Limitations of Leaderless Replication**

Leaderless replication provides **availability**, but sacrifices **strong consistency** unless quorums are properly configured.

### **1. Eventual Consistency**

Without quorum guarantees, reads may return stale or conflicting data.

### **2. Conflict Resolution Complexity**

Developers must understand:

* vector clocks
* merging
* CRDTs

This complicates application logic.

### **3. Write Amplification**

Each write must go to multiple nodes.
Latency increased = max latency among quorum nodes.

### **4. Hot Partitions**

Popular keys may overload certain shard replicas.

### **5. Metadata Overhead**

Tracking versions and hinted handoffs increases storage costs.

---

# **VII. Data Types and CRDTs**

Conflict-free replicated data types (CRDTs) are special data structures that **merge automatically** without conflict:

Examples:

* G-Counter: only increments
* PN-Counter: increments + decrements
* OR-Set: add/remove elements
* LWW-Element-Set
* Map-based CRDTs

CRDTs guarantee **convergence**:

* regardless of write order,
* no need for locks,
* no need for consensus.

These allow powerful collaborative apps:

* distributed counters,
* shopping carts,
* sets of tags,
* document collaboration.

Used in:

* Riak,
* Redis CRDT modules,
* distributed apps like Automerge.

---

# **VIII. Common Databases Using Leaderless Replication**

Systems that use leaderless replication principles:

| System           | Model              |
| ---------------- | ------------------ |
| Amazon Dynamo    | Original AP design |
| Cassandra        | Dynamo-inspired    |
| Riak             | Dynamo-inspired    |
| Voldemort        | KV store           |
| DynamoDB streams | indirect Dynamo    |
| ScyllaDB         | Cassandra-like     |

These systems emphasize:

* fault tolerance,
* low-latency writes,
* global distribution.

---

# **IX. Comparison With Other Replication Models**

| Feature      | Leader–Follower | Multi-Leader | Leaderless   |
| ------------ | --------------- | ------------ | ------------ |
| Writes       | One node        | Many nodes   | Any replica  |
| Failover     | Hard            | Medium       | Easy         |
| Conflicts    | None            | Yes          | Yes          |
| Latency      | Medium          | Low local    | Low local    |
| Consistency  | Strong          | Eventual     | Configurable |
| Availability | Medium          | High         | High         |
| Use case     | OLTP, ACID      | Geo writes   | Global KV    |

Leaderless replication shines when:

* patterns are like a **key-value store**,
* updates are simple and frequent,
* latency is critical,
* partition tolerance matters.

---

# **X. Strong Consistency in Leaderless Systems**

Leaderless systems **can** provide strong consistency **if quorum is respected**:

Example:

* N = 5
* W = 3
* R = 3
* R + W > N => 6 > 5 (true)

This ensures:

* write affects 3 replicas,
* read queries 3 replicas,
* overlap of at least one has latest value.

But:

* latency = slowest node in quorum,
* availability declines with high W and R.

Thus, strong consistency is **possible but expensive**.

---

# **XI. CAP Theorem Interpretation**

Leaderless = typically **AP**:

* Available
* Partition-tolerant

Unified consensus (e.g., Raft) ensures **CP**:

* Consistent
* Partition-tolerant

Leaderless systems often adopt:

* **eventual consistency**
* with tunable consistency via R/W settings.

---

# **XII. Why Leaderless Matters Historically**

Leaderless replication was originally driven by:

* Amazon Dynamo’s need for **high availability under network failures**
  (Black Friday traffic outages)

Dynamo paper:

* emphasized shopping cart merging,
* focused on **business value vs consistency**.

It influenced:

* DynamoDB, Cassandra, Riak,
* NoSQL movement,
* eventual consistency philosophy.

---

# **XIII. Key Takeaway Principle**

> **Leaderless replication removes the single leader bottleneck by allowing any replica to accept writes. Consistency is ensured through quorum reads/writes and versioning mechanisms like vector clocks, read repair, and hinted handoff. This provides high availability and partition tolerance but requires careful conflict resolution and may return stale data.**

Leaderless systems are powerful and scalable, but require:

* understanding of R/W quorum math,
* acceptance of eventual consistency,
* robust conflict merging strategies.