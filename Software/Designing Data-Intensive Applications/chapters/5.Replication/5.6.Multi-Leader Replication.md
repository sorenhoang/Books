## **Topic 5.6 — Multi-Leader Replication**

Multi-leader replication (also called multi-master replication) is a replication architecture in which **multiple nodes accept writes**, and then replicate those writes to each other. Unlike leader–follower replication (single-writer model), where all writes are serialized through one leader, multi-leader replication allows **multiple leaders in different locations** to accept writes concurrently.

This enables **better write availability**, **latency reduction**, and **geographic distribution**, but also introduces **conflict resolution challenges** because writes may happen on different nodes at the same time.

Multi-leader replication is significantly more complex than single-leader replication and must be used deliberately.

---

# **I. Why Multi-Leader Replication Exists**

The standard leader–follower design has limitations:

1. **Single leader = single write bottleneck**

   * All writes go through one node.

2. **Latency across regions**

   * Clients far from leader experience high write latency.

3. **Availability**

   * If leader fails, write availability stops until failover.

Multi-leader replication solves these by:

* placing a leader near each geography,
* allowing writes at local speed,
* replicating changes later.

Example:

```
US Leader <──replicate──> EU Leader
```

Users in US write to US node, users in Europe write to EU node. Each region’s writes propagate to the other.

---

# **II. How Multi-Leader Replication Works**

Each node is both:

* **Leader (writer)**
* **Follower (replica)** of other leaders

Flow:

1. Client writes to Leader A.
2. Leader A records write locally.
3. Leader A asynchronously replicates write to Leader B.
4. Leader B applies write and logs it.

Symmetrically, writes to B replicate back to A.

This creates **circular replication**.

---

# **III. Benefits of Multi-Leader Architecture**

### **1. Higher Availability**

If one leader fails, other leaders still accept writes.
No downtime for failover.

### **2. Reduced Latency**

Clients write to the nearest leader.
Avoids cross-region network delay.

### **3. Offline Capability**

Nodes can accept writes while disconnected and sync back later.
Useful for:

* mobile sync
* branch office apps
* edge computing

### **4. Read Scaling**

Multiple leaders and followers allow horizontal scaling.

---

# **IV. The Major Problem: Conflicts**

The biggest challenge in multi-leader replication is **write conflicts**:

When two leaders accept writes to the **same record simultaneously**, each applies its own write and replicates it to the other.

Without careful resolution, the system diverges.

Example:

```
Initial: x = 0

Leader A: x = x + 5 → x = 5
Leader B: x = x + 7 → x = 7
```

Both writes happen independently, then replicate:

* If A applies B’s write after its own → x = 12
* If B applies A’s write after its own → x = 12
* But if they overwrite → final depends on last write

This is a classic **lost update** or **conflicting update** problem.

Leader–follower avoids this by serializing all writes through one node. Multi-leader must deal with conflicts explicitly.

---

# **V. Conflict Avoidance Strategies**

### **1. Sync Writes (Synchronous Multi-Leader)**

* Leaders coordinate before commit
* Effectively becoming a consensus system
* Not truly multi-leader anymore
* High latency, poor scalability

Rarely used in real systems.

---

### **2. Single-Leader per Record (Partitioning)**

Partition keys so each record has **one authoritative leader**.

Example:

* Based on user ID:

  * Users 1–1000 → Leader A
  * Users 1001–2000 → Leader B

Writes to the same record always go through the same leader.
No conflict.

But:

* Requests must be routed correctly
* Requires deterministic sharding

Used in large-scale distributed SQL (Google Spanner, CockroachDB) but with consensus instead of async replication.

---

### **3. Application-Level Conflict Resolution**

When conflicts arise:

* application merges values,
* or chooses winner.

Conflict resolution strategies include:

#### **Last Write Wins (LWW)**

* store timestamp
* latest timestamp wins.

Easy but may cause **lost updates**.

#### **Causal Merging**

* If both updates are independent, sum them.

Example:

* counters, shopping cart additions.

#### **Custom Merge Logic**

* Domain-specific rules.

Example:

* in a document editing app, merging text
* for bank accounts, summing balances

---

### **4. Conflict Detection**

Database tags conflicting updates:

* keeps both versions,
* lets client or server resolve.

This requires version metadata:

* vector clocks
* conflict-free replicated data types (CRDTs)

---

# **VI. Conflict Resolution Models**

### **1. Convergent (Eventual Convergence)**

All replicas reach same final state.

### **2. Causal Ordering**

Operations aware of cause–effect relationships.

### **3. CRDTs**

Special data types designed to merge without conflicts.
Examples:

* counters
* sets
* maps

CRDTs always converge, regardless of ordering.

Used in:

* Riak
* Redis CRDTs
* Databases for offline-first apps

---

# **VII. Replication Latency and Consistency**

Because leaders replicate asynchronously:

* writes on one leader are not immediately visible on others,
* **eventual consistency** prevails across leaders,
* local reads may be stale compared to remote writes.

This is acceptable for some domains (social feeds) but not for others (banking).

---

# **VIII. Use Cases for Multi-Leader**

### **1. Geo-Distributed Applications**

Users write to nearest region.

### **2. Offline Editing**

Clients/mobile apps record updates offline, sync when online.

### **3. Collaborative Applications**

Users edit shared docs — merge conflicts using CRDT or patch logic.

### **4. Multi-Datacenter Availability**

Each datacenter acts as leader; failover is transparent.

### **5. Legacy Systems**

Built before consensus algorithms matured.

---

# **IX. Challenges and Drawbacks**

* Conflict handling adds complexity
* Merge logic often domain-specific
* Hard to enforce uniqueness constraints globally
* Schema changes harder to coordinate
* Competing writes on same record complicate state
* Latency between leaders increases convergence time
* Write amplification when both leaders replicate everywhere

Systems may appear integrated but diverge under load.

---

# **X. Anti-pattern: Multi-Leader Without Conflict Strategy**

If developers enable multi-leader replication **without thinking**:

* subtle data corruption occurs,
* inconsistencies accumulate,
* replicas disagree,
* debugging becomes hard.

Without a clear conflict resolution model, multi-leader is dangerous.

---

# **XI. Contrast With Other Models**

### **Leader–Follower**

* One writer
* No conflicts
* Strong consistency
* Simpler

### **Multi-Leader**

* Multiple writers
* Conflicts possible
* Eventual consistency
* Complex

### **Leaderless (Next Topic)**

* No fixed leader
* Quorum consensus
* Uses majority for safety
* More predictable than multi-leader async

---

# **XII. Modern Consensus vs Multi-Leader**

With modern consensus systems (Raft, Paxos):

* consensus ensures **only one leader per shard**
* multi-leader is rarely used for strong consistency

Instead:

* leaderful systems are used per shard,
* global cluster behaves as multi-leader,
* but per-key writes are serialized.

Example:

* CockroachDB (Raft per range)
* Spanner (Paxos per partition)

This avoids conflict by embracing consensus.

---

# **XIII. Key Takeaway Principle**

> **Multi-leader replication allows multiple nodes to accept writes for higher availability and lower latency, but introduces the risk of conflicting updates. The core challenge is conflict detection and resolution, which often requires domain-specific logic or CRDT-based merging.**

Use multi-leader only when:

* strong consistency is not required,
* or conflicts are manageable,
* or consistency can be partitioned per key,
* or consensus per shard is employed.

Most production systems today use:

* **single-leader** for transactional workloads,
* **multi-leader** for offline/mobile sync,
* **leaderless consensus** for distributed NoSQL.
