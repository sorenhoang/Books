## **Topic 5.1 — Leaders and Followers**

Replication is the process of **copying data across multiple machines** (nodes) so that the system continues functioning if any one node fails, and so that data can be served from multiple locations for performance and scalability. The simplest and most widely deployed replication architecture is the **leader–follower model** (also called **primary–secondary**, **master–slave**, or **source–replica** replication, depending on the database terminology).

In this model, **one node is responsible for accepting writes**, and **other nodes replicate the data** from the leader. Reads can happen on the leader or followers, depending on consistency needs. This architecture is used by most relational databases (PostgreSQL, MySQL, Oracle), many NoSQL systems, and also forms the basis for more complex replication patterns.

---

# **I. Core Idea of Leader–Follower Replication**

### **How it works**

1. **Client sends write request**
   → to the **Leader** node.

2. The Leader **executes the write**
   → updates its local database state.

3. The Leader **records the write in a replication log**
   → a sequential record of changes.

4. Followers **receive the log entries**,
   → and **apply the same writes**, in the same order.

This ensures all nodes eventually reach the **same state**.

Once the replication is applied, followers contain a copy of the data identical to the leader — preserving the logical order of writes.

---

# **II. Why Order Matters**

To maintain correctness, all followers must apply writes in **exactly the same order** as the leader. Otherwise:

* conflicting updates may lead to divergence,
* foreign key relationships may break,
* unique constraint enforcement may differ,
* secondary indexes may become inconsistent.

This is why the leader’s replication log is **append-only and ordered**, similar to a transaction log.

---

# **III. Benefits of Leader-Based Replication**

The model has several strong properties that make it easy to reason about:

### **1. Simplicity Guarantees Consistency**

Since writes are serialized through one node, the system naturally avoids write-write conflicts.

### **2. Clear Ownership of Writes**

No confusion about who “wins” — the leader is the source of truth.

### **3. Easy Change Propagation**

Followers just apply changes through log shipping or streaming.

### **4. Scalable Reads**

Followers can serve read queries, allowing systems to:

* offload read traffic from the leader,
* scale horizontally for read-heavy workloads.

This model is popular for:

* analytics replicas,
* global read replicas,
* caching and reporting clusters.

---

# **IV. Replication Techniques**

There are two broad approaches to transferring data from leader to followers:

### **1. Physical (Binary) Shipping**

* Transfers disk blocks or file segments.
* Low-level replication used in PostgreSQL WAL-shipping.
* Efficient and ordered.

### **2. Logical Replication**

* Transfers higher-level operations:

  * “INSERT INTO X”
  * “UPDATE balance SET = balance – 50 WHERE id = 123”
* More flexible across versions and schemas.
* Allows filtering or transforming replication.

Logical replication enables:

* upgrading followers to different database versions,
* selectively replicating tables,
* replication across different storage engines.

---

# **V. Replication Lag (Asynchronous Nature)**

Followers **consume data from the leader with a delay**. This delay is called **replication lag**.

Causes include:

* network delay,
* heavy write workload on leader,
* slow follower hardware,
* follower replay backlogs.

As a result:

* followers may temporarily not have the most recent data,
* reading from a follower may return **stale data**.

This impacts consistency guarantees:

* read-your-own-writes may fail,
* monotonic reads may break.

Later topics (5.2) explore synchronous vs asynchronous replication, which define how lag is handled.

---

# **VI. Read Strategies in Leader–Follower Architectures**

There are two primary models for reads:

### **A. All Reads on Leader**

* Strong consistency.
* Simplest model.
* Limited scalability.

### **B. Followers Serve Reads**

* High scalability for reads.
* Lower latency for geographically distributed clients.
* But introduces **eventual consistency** for reads.

Systems may allow clients to specify:

* strong read,
* stale read,
* consistent read-after-write,
  depending on needs.

---

# **VII. Maintaining Followers**

Followers keep up-to-date with the leader using a **replication log**, which is separate from the internal write-ahead log, but conceptually similar.

Common implementation patterns:

* **WAL (Write-Ahead Log) streaming**: followers tail the leader’s WAL.
* **Change Data Capture (CDC)**: systems like Debezium capture changes and push elsewhere.
* **Incremental snapshot + log replay**: used when followers join later.

When a follower falls behind:

* it may catch up by replaying logs,
* or take a fresh snapshot and resume replication.

---

# **VIII. Failover and Leadership Change**

Leader–follower design must handle **leader failures**.

Typical steps:

1. Detect leader failure (via timeouts, heartbeats).
2. Promote a follower to become the new leader.
3. Redirect writes to the new leader.
4. Other nodes reconfigure state to follow the new leader.

Complications arise:

* **some data may not have been replicated yet**, causing inconsistencies.
* **split brain** could occur if multiple nodes think they are the leader.

This requires consensus or coordination protocols like:

* Raft,
* Paxos,
* Zookeeper,
* etcd.

These guarantee **only one leader is active** at a time.

---

# **IX. Why Leader–Follower is the Default Model**

Most databases use it because:

1. **Easy mental model:**
   One node controls writes. Followers copy state.

2. **Natural fit for ACID transactions:**
   Serializing writes across a single node simplifies consistency.

3. **Predictable conflict resolution:**
   No multi-master race conditions.

4. **High-performance reads:**
   Followers provide horizontal scaling.

5. **Operational simplicity:**
   Leader change is a well-understood process.

This model provides a **balance between consistency and availability**, which fits many real-world workloads.

---

# **X. Limitations of Leader–Follower Replication**

Despite its popularity, it has drawbacks:

### **1. Single write bottleneck**

Leader must process **all** writes, limiting throughput.

### **2. Leader failure impact**

During failover:

* Some writes may be lost,
* Temporary unavailability,
* Risk of inconsistencies.

### **3. Replication lag**

Eventual consistency for follower reads.

### **4. Geographic latency**

If clients are globally distributed:

* remote clients may incur latency sending writes to one location.

---

# **XI. Real-World Systems Using Leader–Follower**

| Technology | Replication Style                 |
| ---------- | --------------------------------- |
| PostgreSQL | Leader + WAL streaming            |
| MySQL      | Primary + Replicas (binlog-based) |
| MongoDB    | Replica sets (single primary)     |
| Oracle RAC | Leader-based coordination         |
| MariaDB    | Primary–Replica                   |
| Redis      | Replication streams               |

Even modern distributed SQL systems (e.g., CockroachDB) simulate a virtual leader per range or shard, though they use consensus to elect it.

---

# **XII. Conceptual Summary**

Leader–Follower is fundamentally about **serializing writes through one node** to:

* ensure a consistent order,
* replicate changes across followers,
* scale reads horizontally,
* preserve a single source of truth.

It trades off:

* **write scalability** and **failure complexity**
  for:
* **predictable consistency** and **design simplicity**.

This model builds the foundation for the rest of the chapter:

* synchronous vs asynchronous replication (5.2),
* failover handling (5.3),
* log details (5.5),
* multi-leader variants (5.6),
* and leaderless models (5.7).

---

## **Key Takeaway Principle**

> **Leader–Follower replication serializes all writes through a single leader node and replicates them as an ordered log to follower nodes for read scaling and fault tolerance.**
> It is the simplest and most widely adopted replication model, balancing consistency, performance, and operational simplicity.