# **MapReduce and Distributed Filesystems**

MapReduce is a programming framework that behaves similarly to Unix tools but is distributed across thousands of machines. It works in tandem with distributed filesystems, such as HDFS (Hadoop Distributed File System), which is based on the shared-nothing principle and uses commodity hardware rather than specialized storage appliances.

### **MapReduce Job Execution**
MapReduce breaks data processing into distinct stages, reading from and writing to a distributed filesystem.

*   **The MapReduce Pattern:**
    1.  **Read input:** The input directory is broken into records (e.g., lines in a log file).
    2.  **Map:** A user-defined mapper function extracts a key and value from each input record,.
    3.  **Sort:** The framework implicitly sorts all key-value pairs by key.
    4.  **Reduce:** A user-defined reducer function iterates over the sorted key-value pairs; values for the same key are adjacent, allowing for efficient aggregation,.
*   **Distributed Execution:**
    *   **Data Locality:** The scheduler attempts to run map tasks on the machine that stores the input file replica to save network bandwidth.
    *   **The Shuffle:** The process of partitioning data by reducer, sorting, and copying partitions from mappers to reducers.
    *   **Fault Tolerance:** MapReduce tolerates the failure of individual map or reduce tasks by retrying them without affecting the job as a whole.
*   **MapReduce Workflows:**
    *   Complex problems are solved by chaining jobs, where the output of one job becomes the input of the next.
    *   Workflow schedulers (e.g., Oozie, Airflow) manage dependencies, ensuring a job starts only after its inputs are ready.

### **Reduce-Side Joins and Grouping**
When a job processes associations between records (like foreign keys), MapReduce handles the logic in the reducers.

*   **Sort-Merge Joins:** Mappers extract the join key (e.g., user ID) from all input datasets. The framework partitions and sorts these by key, bringing all related records to the same reducer call,.
*   **Secondary Sort:** To ensure a specific record order within the reducer (e.g., the user profile record appearing before activity logs), applications can use a secondary sort.
*   **Grouping:** Similar to joins, grouping brings all records with the same key to one place for aggregation (e.g., counting, summing).
*   **Handling Skew:**
    *   **Linchpin Objects:** Hot keys (e.g., celebrities in a social network) can cause significant skew, where one reducer processes much more data than others.
    *   ** mitigation:** Algorithms like skewed joins (Pig) or sharded joins (Crunch) use sampling or randomization to spread the work of hot keys across multiple reducers.

### **Map-Side Joins**
If assumptions can be made about the input data (e.g., size or sorting), joins can be performed entirely in the mapper, avoiding the expensive shuffle and sort phases.

*   **Broadcast Hash Joins:** Used when joining a large dataset with a small one that fits in memory. The small dataset is broadcast to all mappers, which load it into a hash table for lookups.
*   **Partitioned Hash Joins:** Used when both inputs are partitioned in the same way (same number of partitions, same hash function). Mappers read corresponding partitions from both sides.
*   **Map-Side Merge Joins:** Used when inputs are both partitioned and sorted. Mappers merge the files incrementally without loading them entirely into memory.

### **The Output of Batch Workflows**
Batch processing is neither transaction processing (OLTP) nor analytics (OLAP); its output is typically a derived data structure.

*   **Search Indexes:** MapReduce is efficient for building full-text search indexes (e.g., Lucene) over a fixed set of documents. Mappers partition documents, and reducers build index files.
*   **Key-Value Stores:** Batch jobs can build database files (e.g., for Voldemort or HBase) that are then loaded into read-only servers for low-latency querying.
*   **Philosophy of Batch Output:**
    *   **Immutability:** Inputs are treated as immutable, and outputs are completely replaced. This allows for experimentation and easy rollback if bugs are introduced.
    *   **Human Fault Tolerance:** Because code can be rolled back and jobs rerun, feature development is faster and less risky than in systems with irreversible side effects.

### **Comparing Hadoop to Distributed Databases**
Hadoop is comparable to a distributed version of Unix, while Massively Parallel Processing (MPP) databases focus on parallel execution of analytic SQL.

*   **Diversity of Storage:**
    *   **MPP Databases:** Require careful up-front modeling and data import into a proprietary format.
    *   **Hadoop:** Allows indiscriminately dumping data (the "sushi principle": raw data is better) and defining schemas later (schema-on-read), facilitating data integration.
*   **Diversity of Processing:** Hadoop enables SQL (Hive), batch processing (MapReduce), and other models (Spark) to run on the same cluster, accessing the same files.
*   **Fault Tolerance and Resource Utilization:**
    *   **MPP:** Prioritizes performance; aborts queries if a node fails. Best for short-running queries.
    *   **MapReduce:** Prioritizes fault tolerance and resource utilization. Designed for an environment (like Google's) where low-priority batch jobs are frequently preempted to free resources for production services,.