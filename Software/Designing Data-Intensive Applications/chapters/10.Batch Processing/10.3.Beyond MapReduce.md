# **Beyond MapReduce**

While MapReduce was a major step forward for large-scale data processing, it is just one of many possible programming models. It is robust and capable of processing vast amounts of data, but it is also difficult to implement complex jobs with directly, and its execution model—specifically the way it handles intermediate state—can lead to poor performance for certain types of processing. This has led to the development of new execution engines and high-level programming models.

### **Materialization of Intermediate State**

In a MapReduce workflow, the output of one job becomes the input of the next. This requires the first job to completely finish and write its output to the distributed filesystem (like HDFS) before the next job can begin. This process of writing intermediate state to durable storage is called **materialization**.

*   **Downsides of MapReduce Materialization:**
    *   **Latency:** A job cannot start until all tasks in the preceding job have completed. If a few "straggler" tasks take a long time, the entire workflow is delayed.
    *   **Redundancy:** Mappers often just read back the file written by the previous reducer to prepare it for the next sorting stage, which is redundant work.
    *   **I/O Overhead:** Intermediate state is replicated across nodes in HDFS, which is unnecessary for temporary data.

*   **Dataflow Engines (Spark, Tez, Flink):**
    These engines handle an entire workflow as a single job rather than breaking it into independent subjobs. They act like distributed Unix pipes, processing data incrementally rather than fully materializing it.
    *   **Operators:** Instead of strict map and reduce steps, these engines use flexible **operators** that can be connected in various ways (e.g., repartitioning and sorting for joins, or just partitioning without sorting).
    *   **Advantages:**
        *   **Better Locality:** The scheduler can make optimizations, such as placing a consumer task on the same machine as the producer to exchange data via shared memory.
        *   **Reduced I/O:** Intermediate state is often kept in memory or written to local disk, avoiding the overhead of replication.
        *   **Pipelining:** Operators can start executing as soon as input is ready, without waiting for the entire preceding stage to complete.

*   **Fault Tolerance in Dataflow Engines:**
    *   Because intermediate state is not written to HDFS, dataflow engines cannot simply restart a failed task from the last file checkpoint.
    *   Instead, they assume input data is immutable and **recompute** lost data by tracking the ancestry of operations (e.g., Spark's Resilient Distributed Datasets or RDDs).
    *   This approach works best if operators are **deterministic**; if they are nondeterministic, downstream operators may also need to be killed and restarted to resolve inconsistencies.

### **Graphs and Iterative Processing**

Graph algorithms (like PageRank) are often **iterative**, meaning they traverse edges and update vertices repeatedly until a condition is met (convergence).

*   **MapReduce Limitations:** MapReduce is inefficient for iterative algorithms because it must read the entire dataset and produce a completely new output dataset for every single iteration, even if only a small part of the graph changed.
*   **The Pregel Model (Bulk Synchronous Parallel):**
    *   **Vertex-Centric:** A vertex "sends messages" to other vertices along the edges of a graph.
    *   **Memory State:** Unlike MapReduce, vertices remember their state in memory from one iteration to the next, so they only process new incoming messages.
    *   **Fault Tolerance:** Achieved by periodically checkpointing the state of all vertices to durable storage at the end of an iteration. If a node fails, the computation rolls back to the last checkpoint.
*   **Parallel Execution:** The graph is partitioned across machines. However, partitioning graphs efficiently is difficult; often arbitrary ID-based partitioning is used, which can result in high cross-machine communication overhead. If a graph fits in memory on a single machine, single-machine algorithms are often faster than distributed ones.

### **High-Level APIs and Languages**

As the physical operation of large-scale batch processing has become a solved problem, focus has shifted to improving programming models and efficiency.

*   **Declarative Query Languages:**
    *   High-level languages (Hive, Pig, Spark DataFrame) allow users to express computations as relational operators (joins, groups, filters) rather than code.
    *   **Query Optimizers:** Frameworks can use cost-based optimizers to automatically decide which join algorithm to use (e.g., broadcast vs. partitioned hash join) and reorder operations for efficiency.
    *   **Vectorized Execution:** Engines like Hive and Spark can optimize inner loops by processing data in tight, CPU-cache-friendly loops (vectorization), avoiding the overhead of function calls for every record.

*   **Specialization:**
    *   Batch processing is expanding beyond business intelligence into domains like **machine learning** (e.g., Mahout, MADlib) and **spatial algorithms** (e.g., k-nearest neighbors).
    *   The flexibility of batch processing allows these diverse workloads to run on the same shared cluster and access the same files, reducing the need to move data between specialized systems.