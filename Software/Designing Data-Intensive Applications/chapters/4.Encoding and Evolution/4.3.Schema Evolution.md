## **Topic 4.3 ‚Äî Schema Evolution**

Schema evolution refers to how data structures and serialization formats change over time while maintaining compatibility between old data, new data, old code, and new code. In real systems, requirements evolve: new fields are added, old fields become obsolete, types change, and applications update independently. Without a proper approach to schema evolution, changes can break consumers, corrupt data, or cause failures across distributed applications.

As systems scale and become more distributed, schema evolution becomes essential‚Äînot optional. The goal is to allow software to evolve **without requiring an instantaneous, synchronized migration** of all data producers, consumers, and stored records. This ability is especially crucial in event-driven architectures, long-lived storage, and microservices.

---

### **Why Schema Evolution Matters**

Real systems rarely remain static. Examples:

* New features require new data fields.
* Old fields become deprecated.
* Multiple services owned by different teams consume the same data.
* Stored records must remain readable years later.

Without schema evolution discipline, systems become fragile and resistant to change.

Example failure case:

> A producer adds a required field but consumers don‚Äôt expect it ‚Üí parsing failure ‚Üí production outage.

Schema evolution prevents such compatibility failures.

---

### **Key Types of Compatibility**

There are three primary compatibility concerns:

| Compatibility Type              | Meaning                                      |
| ------------------------------- | -------------------------------------------- |
| **Backward Compatibility**      | New code can read data written by old code   |
| **Forward Compatibility**       | Old code can read data written by newer code |
| **Bidirectional Compatibility** | Both backward and forward compatibility hold |

Backward compatibility is most common; forward compatibility is essential when data persists longer than code versions.

---

### **Schema Evolution Rules in Practice**

To preserve compatibility, evolution rules must govern how schemas may change.

#### üéØ **Safe changes generally include:**

* Adding an optional field
* Providing default values for new required fields
* Removing unused optional fields
* Changing field order (in schema-based formats)

#### ‚ùå **Breaking changes include:**

* Removing a required field
* Changing a field type in incompatible ways
* Renaming fields without alias or mapping rules
* Requiring new mandatory fields without defaults

Whether a change is safe depends on whether the encoding format supports schema metadata.

---

### **Handling Missing or Extra Fields**

During evolution:

* **Old messages may lack newly added fields**
* **New messages may contain fields unknown to old readers**

Different formats handle this differently:

| Format Type                                  | Behavior                            |
| -------------------------------------------- | ----------------------------------- |
| Text-based (JSON, XML)                       | Unknown fields are usually ignored  |
| Binary schema-based (Avro, Protobuf, Thrift) | Schema explicitly controls behavior |
| Strict binary formats (raw byte streams)     | Any mismatch breaks parsing         |

A good encoding system treats unknown fields as optional and ignorable.

---

### **Schema Evolution for Stored Data vs Streaming Data**

Schema evolution challenges differ based on how data is used:

#### üóÑÔ∏è **Stored Data (Databases, Files)**

Old data may persist indefinitely, so new code must continue to interpret it.

Key techniques:

* Backfill or transform old data
* Lazy migration (update on read/write)
* Schema registry for version tracking

#### üì° **Streaming Data (Kafka, Event Logs)**

Events are immutable, so future schemas must continue to read historical messages.

Producers and consumers evolve independently ‚Üí schema registry becomes essential (e.g., Kafka + Avro Schema Registry).

---

### **Schema Migration Approaches**

There are several common strategies for changing data formats:

| Approach                        | Description                                    | Trade-offs                                   |
| ------------------------------- | ---------------------------------------------- | -------------------------------------------- |
| **In-place migration**          | Update all existing data to new schema         | Simple result but expensive / downtime risk  |
| **Write-new, read-old-and-new** | Application handles both formats               | Flexible, but adds logic complexity          |
| **Lazy migration**              | Migrate items gradually when accessed          | Avoids cost spikes; requires dual read logic |
| **Dual write format**           | Store data in both schema versions temporarily | Useful for rollbacks                         |

Distributed systems commonly use **"write new + read both"** approach.

---

### **Schema Versioning Patterns**

There are three common versioning strategies:

* **Implicit Versioning** (schema inferred based on field presence ‚Äî common in JSON)
* **Explicit Field Versioning** (each schema version has its own identifier)
* **Schema ID Registry** (Kafka, Protobuf, Avro deployments)

Using a **schema registry** ensures:

* Consumers know which schema applies to which record
* Compatibility rules can be enforced automatically
* Multiple versions can coexist safely

---

### **Schemas and Serialization Formats**

Formats differ widely in how they support schema evolution:

| Format            | Evolution Support                                           |
| ----------------- | ----------------------------------------------------------- |
| JSON / XML        | Flexible but no enforcement; requires conventions           |
| CSV               | No schema metadata; fragile                                 |
| Protobuf / Thrift | Good support with numeric tags, optional fields             |
| Avro              | Excellent for streaming; stores schema IDs externally       |
| Parquet / ORC     | Structured metadata ‚Üí strong schema evolution for analytics |

Schema-based binary formats are designed specifically to handle evolution safely.

---

### **Common Real-World Guidelines**

Best practices include:

1. **Never remove or rename fields without compatibility strategy**
2. **Always provide defaults for new required fields**
3. **Document and register schema changes**
4. **Ensure producers and consumers evolve independently**
5. **Prefer additive, backward-compatible changes**

Teams treating schema changes like code changes avoid long-term technical debt.

---

### **Key Principle**

Schema evolution enables systems to change and grow without breaking existing data or consumers. The ability to maintain **backward and forward compatibility** is essential for long-lived data, distributed systems, and event-driven architectures. Schema evolution is not only a technical problem ‚Äî it requires engineering discipline, governance, and consistent patterns across teams.