## **Topic 7.4 — Serializability and Performance**

Serializable isolation is the strongest isolation level—but also the most expensive.
This topic explains:

* why serializability impacts performance,
* where bottlenecks come from,
* how systems optimize serializable isolation,
* trade-offs between pessimistic and optimistic approaches,
* and why distributed serializability is even harder.

The goal is to understand **how to achieve correct behavior with minimal performance loss**.

---

# **I. Why Serializable Isolation Is Expensive**

Serializable requires that:

> **The outcome must be equivalent to running all transactions in a single serial order.**

To maintain this guarantee, the system must **detect and prevent dangerous interleavings**.

This leads to overhead in:

1. **Locking or conflict detection**
2. **Coordinating concurrent operations**
3. **Tracking read and write sets**
4. **Aborting or delaying transactions**

Because of these needs, serializable isolation has higher:

* CPU cost
* memory cost
* metadata tracking
* contention
* abort rate

Yet modern databases have developed techniques to minimize the overhead.

---

# **II. Performance Cost Sources**

### **1. Lock Contention (in 2PL systems)**

Strict two-phase locking requires:

* reads acquire shared locks,
* writes acquire exclusive locks,
* locks held until commit.

Leads to:

* blocking reads,
* deadlocks,
* lower concurrency,
* unpredictable latency.

### **2. Abort Overhead (in optimistic systems / SSI)**

Serializable Snapshot Isolation (SSI) detects anomalies and **aborts transactions** that would violate serializability.

High concurrency → more dependency cycles → higher abort rate.

### **3. Read/Write Set Tracking**

To detect dangerous patterns, systems must track:

* which transactions read which keys,
* which transactions write which keys,
* timestamp ordering,
* dependency graphs.

This consumes CPU and memory.

### **4. Coordination (especially in distributed systems)**

Serializable isolation across shards requires:

* synchronized timestamps,
* two-phase commit,
* consensus,
* additional checks for cross-shard conflicts.

Coordination = latency.

---

# **III. Comparison: Pessimistic vs Optimistic Approaches**

## **1. Pessimistic (Locking – 2PL)**

### **Advantages**

✔ No aborts due to conflicts
✔ Good for high-contention workloads
✔ Predictable behavior

### **Disadvantages**

❌ Blocking
❌ Deadlocks
❌ Poor performance for read-heavy workloads
❌ Difficult in distributed databases

Locking performs well **only when write contention is high**.

---

## **2. Optimistic (SSI / OCC)**

### **Advantages**

✔ No blocking reads
✔ High concurrency for read-heavy workloads
✔ Good in distributed systems

### **Disadvantages**

❌ Aborts under high contention
❌ Extra metadata tracking
❌ Harder to reason about performance

Optimistic concurrency works best **when conflicts are rare**.

---

# **IV. Serializable Snapshot Isolation (SSI) Overheads**

SSI (used in PostgreSQL & CockroachDB) introduces several overheads:

### **1. Dependency Tracking**

System tracks:

* "rw-conflicts" (read-write),
* "ww-conflicts" (write-write),
* "dangerous structures" (cycles),
* transaction timestamps.

### **2. Memory Overhead**

Extra metadata to store:

* predecessor information,
* read/write sets,
* locks on committed transactions.

### **3. Transaction Aborts**

When a dependency cycle forms (A reads X, B writes X; B reads Y, A writes Y), one transaction must abort.

Aborts waste work.

### **4. Commit-Wait**

To ensure external consistency (in CockroachDB/Spanner), a transaction waits until its timestamp is guaranteed safe.

Adds latency.

---

# **V. Techniques to Improve Serializable Performance**

Even though serializability is expensive, many systems employ techniques to **reduce overhead**.

---

## **Technique 1: MVCC (Multi-Version Concurrency Control)**

MVCC avoids blocking reads:

* Readers read a snapshot
* Writers create new versions
* Readers and writers don’t block each other

Improves read-heavy workloads significantly.

Used in:

* PostgreSQL
* Oracle
* MySQL InnoDB
* CockroachDB
* Spanner

---

## **Technique 2: Optimistic Concurrency Control (OCC)**

Allows transactions to run freely → validate at commit → abort if conflicts.

Good when:

* few conflicts
* short transactions
* high read/write ratio

Bad when:

* contention is high
* long transactions (more work wasted upon abort)

---

## **Technique 3: Timestamp Ordering**

Assign each transaction a timestamp; ensure operations respect timestamp ordering.

Used in:

* CockroachDB
* Spanner

Benefits:

* clear serialization order
* simple conflict rules
* distributed-friendly

Challenges:

* requires global timestamp synchronization (Spanner relies on TrueTime)

---

## **Technique 4: Predicate Locking or Index-Range Locks**

Needed to prevent phantom anomalies.

But locking entire ranges is expensive → increases contention.

Modern DBs approximate using:

* index gap locks (MySQL),
* predicate locks (PostgreSQL SSI).

---

## **Technique 5: Parallel Execution and Splitting Transactions**

Databases optimize work inside transactions:

* parallel scan execution,
* pipelined operators,
* prefetching,
* batching writes.

These reduce the cost of maintaining serializability.

---

## **Technique 6: Partition-Level Serializability**

Some distributed systems make each **partition** serializable, then rely on higher-level logic to manage cross-partition correctness.

Examples:

* DynamoDB only guarantees per-key serializability
* Cassandra LWT is per-partition key
* Spanner uses per-range Paxos groups

Cross-partition transactions can still be expensive.

---

# **VI. Distributed Serializability: Why It’s Hard**

Serializable isolation in a **distributed** database requires dealing with:

### **1. Network partitions**

Which can cause different nodes to make conflicting assumptions.

### **2. Clock skew**

Makes timestamp ordering difficult unless compensated (e.g., TrueTime).

### **3. Cross-shard coordination**

Every shard must agree on commit order.

### **4. Distributed deadlocks (in locking systems)**

### **5. Replication lag**

Followers may return stale data unless carefully managed.

### **6. Two-phase commit (2PC)**

Distributed commit protocol adds:

* latency
* new failure modes
* need for coordinator recovery

Serializable isolation may multiply these difficulties.

---

# **VII. External Consistency (Stronger Than Serializable)**

Some systems (Spanner, CockroachDB) aim for **external consistency**, meaning:

> Real-time order of transactions is preserved in the serialization order.

If T1 commits before T2 starts, T2 must observe T1’s effects.

This requires:

* global timestamps,
* commit wait periods,
* tightly-synchronized clocks.

Spanner uses TrueTime with bounded uncertainty.
CockroachDB uses hybrid logical clocks.

This further increases latency.

---

# **VIII. When Serializable Isolation Performs Well**

Serializable performs well when:

✔ Transaction conflicts are rare
✔ Workload is read-heavy
✔ Transactions are short
✔ System uses MVCC + optimistic approaches
✔ Workload benefits from correctness guarantees
✔ Hardware/network resources are plentiful

Examples:

* financial systems
* state machine transitions
* inventory correctness
* system of record databases

---

# **IX. When Serializable Isolation Performs Poorly**

Serializable performs poorly when:

❌ High write contention
❌ Long-running transactions
❌ Hot keys or hotspots
❌ Large read-modify-write workloads
❌ Distributed cross-shard transactions
❌ Lock-heavy legacy code

In such cases, simpler isolation levels or redesigning workload patterns may be needed.

---

# **X. Comparing Isolation Levels and Performance**

| Isolation Level    | Performance | Consistency |
| ------------------ | ----------- | ----------- |
| Read Uncommitted   | ⭐⭐⭐⭐⭐       | ⭐           |
| Read Committed     | ⭐⭐⭐⭐        | ⭐⭐          |
| Snapshot Isolation | ⭐⭐⭐         | ⭐⭐⭐         |
| Serializable (SSI) | ⭐⭐          | ⭐⭐⭐⭐⭐       |
| Serializable (2PL) | ⭐           | ⭐⭐⭐⭐⭐       |

Serializable (except in systems optimized for it) tends to be slower than weak isolation levels.

---

# **XI. Engineering Guidelines for Developers**

If your DB uses **Serializable**:

### ✔ Keep transactions short

Long transactions increase abort likelihood.

### ✔ Avoid hot rows

High contention causes many rollbacks.

### ✔ Avoid large scans inside transactions

They expand the read set → more conflicts.

### ✔ Design schemas to reduce cross-partition dependencies

Avoid multi-shard operations where possible.

### ✔ Use optimistic conflict resolution for high-throughput systems

Design updates to be commutative when possible.

### ✔ Use SELECT FOR UPDATE only when necessary

Unnecessary locks ruin concurrency.

---

# **XII. Key Takeaways for Topic 7.4**

### **1. Serializable isolation is safe but expensive.**

It eliminates all anomalies but requires careful coordination and detection of conflicts.

### **2. Pessimistic locking vs optimistic concurrency trade-off:**

* Pessimistic: blocking, deadlocks, predictable.
* Optimistic: aborts, high throughput, works with MVCC.

### **3. MVCC + SSI is the best practical approach**

Adopted by PostgreSQL and CockroachDB.

### **4. Distributed serializability is significantly harder**

Requires timestamps, consensus, 2PC, and careful conflict resolution.

### **5. Performance depends on workload**

Serializable isolation is fast in read-heavy, low-contention workloads, but slow under write contention.

### **6. Systems like Spanner and CockroachDB optimize serializable isolation with timestamp ordering and global clock sync.**
