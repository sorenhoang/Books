## **Topic 6.1 — Partitioning and Replication**

Partitioning (also called **sharding**) is the technique of **splitting a large dataset into smaller, manageable pieces** and distributing them across multiple machines. Each piece is called a **partition** (or *shard*). Replication, which you learned in Chapter 5, ensures **multiple copies** of each partition exist to tolerate failures.

This topic explains how **partitioning interacts with replication** to form the foundation of large-scale distributed databases.

---

# **I. Why Partitioning Is Necessary**

As data volume grows, a single machine eventually becomes insufficient due to:

* storage capacity limits,
* memory constraints,
* I/O bottlenecks,
* CPU saturation,
* network bandwidth limits.

Replication alone **cannot** solve scaling:
replication **copies** the same data to multiple nodes but does **not** expand the total dataset capacity.

Therefore, systems must partition data so **each node handles only a subset** of the data.

---

# **II. Relationship Between Partitioning and Replication**

In a real distributed database:

* You **partition** the dataset to scale,
* You **replicate** each partition for fault tolerance.

Together they form:

```
Partitioning → scalability
Replication → availability + durability
```

The system becomes a grid of **partition × replica** nodes.

For example:

```
Partition 1: Node A, Node B, Node C
Partition 2: Node D, Node E, Node F
Partition 3: Node G, Node H, Node I
```

Each partition has multiple replicas, typically following a leader–follower or leaderless model.

---

# **III. Partitioning + Replication = Data Distribution Matrix**

A typical distributed database layout:

| Partition | Replica Leader | Replica Followers |
| --------- | -------------- | ----------------- |
| P1        | N1             | N2, N3            |
| P2        | N4             | N5, N6            |
| P3        | N7             | N8, N9            |

This structure allows:

* horizontal scaling → more partitions
* reliability → replication of each partition
* concurrent load → each partition handles part of the traffic

Nodes can host multiple partitions:

```
Node A → P1 leader, P3 follower
Node B → P1 follower, P2 leader
Node C → P2 follower, P3 leader
```

Balanced placement is crucial for avoiding hotspots.

---

# **IV. Each Partition Has Its Own Replication Group**

Replication logic (from Chapter 5) applies **per partition**.

### **Leader–Follower per Partition**

Each partition has:

* one **leader** for writes,
* multiple **followers** for reads,
* its own failover process.

Example (Cassandra-like per-shard behavior with leader-based replication):

```
Shard 1 → Leader: Node A  
Shard 2 → Leader: Node B  
Shard 3 → Leader: Node C  
```

### **Leaderless per Partition**

Systems like Dynamo or Cassandra use quorum-based replication **within each partition**.

Each partition:

* has *N* replicas,
* responds to *R* reads and *W* writes,
* converges using repair processes.

---

# **V. Importance of Partitioning for Scalability**

Partitioning enables databases to handle:

### **1. More Data**

Because storage is distributed.

### **2. More Writes**

Writes are spread across multiple leaders (per partition), avoiding the bottleneck of a single leader.

### **3. More Reads**

Followers for each partition increase total read throughput.

This design is used by:

* Cassandra
* MongoDB
* Elasticsearch
* DynamoDB
* CockroachDB
* Spanner

---

# **VI. Partition Independence**

Each partition handles its own:

* replication,
* failover,
* log,
* consensus or quorum operations.

By isolating partitions, systems:

* reduce global coordination,
* avoid large-scale cascaded failures,
* limit performance bottlenecks.

Example:
Consensus protocols (Raft, Paxos) run **per partition**, not globally.

This allows hundreds/thousands of partitions to scale linearly.

---

# **VII. Challenges of Combining Partitioning and Replication**

### **1. Uneven Load (Hot Partitions)**

Popular keys cause hotspots.

Example:

* A celebrity user ID in a social network gets disproportionate traffic.
* Hash partitioning helps but cannot always avoid skew.

### **2. Cross-Partition Transactions**

Transactions touching multiple partitions require:

* two-phase commit,
* distributed locking,
* complex coordination.

This significantly increases complexity.

### **3. Rebalancing**

When adding nodes, partitions must move:

* Leaders transferred,
* Followers copied,
* Data redistributed.

Rebalancing must be done **without downtime**.

### **4. Routing Requests**

The system must know:

* which partition holds which key,
* which replica to query,
* how to route around failures.

Various routing strategies (covered in Topic 6.5).

---

# **VIII. Partition Count & Replication Factor**

There are two axes of horizontal scaling:

* **Partition count** → how many subsets the data is divided into
* **Replication factor (RF)** → how many copies of each subset

Total nodes required:

```
nodes ≥ partitions × RF / average_partitions_per_node
```

Increasing **partitions** improves parallelism.
Increasing **replicas** improves availability.

Example:

```
Partition count = 100  
Replication factor = 3  
→ Total replicas = 300
```

These replicas are spread evenly across nodes.

---

# **IX. Multi-Datacenter (Geo) Partitioning**

Replication and partitioning in multi-region environments follow patterns:

### **Leader-Based per Region**

Each region has its own leaders for local partitions.

### **Global Partition Mapping**

Some partitions may be:

* pinned to one region,
* or globally replicated,
* or dynamically moved.

Geo-distribution increases:

* latency variability,
* consistency challenges,
* conflict complexity.

Systems like Spanner use **TrueTime + Paxos per shard** to maintain global consistency despite partitions.

---

# **X. Partitioning + Replication in Real Systems**

### **Cassandra**

* Leaderless, quorum-based replication.
* Consistent hashing for partitioning.
* Replicas distributed across nodes and racks.

### **MongoDB Sharded Clusters**

* Sharded keys determine partitions.
* Each shard has a leader (primary) + followers (secondaries).
* Config servers track shard metadata.

### **Elasticsearch / OpenSearch**

* Index split into shards.
* Each shard has primary + replica shards.
* Automated rebalancing.

### **DynamoDB**

* Fully managed partitioning.
* Internal Dynamo-style replication.
* Partitions autoscale with throughput.

### **CockroachDB**

* Range-based partitions.
* Raft consensus per range.
* Global distribution with strong consistency.

These systems rely heavily on **partition + replication** interplay to scale horizontally.

---

# **XI. Summary of Key Principles**

### **1. Partitioning enables scalability**

* Data split across nodes
* Load distributed across partitions

### **2. Replication provides fault tolerance**

* Each partition replicated for safety and availability

### **3. Each partition operates independently**

* Own leader/quorum
* Own log
* Own failover

### **4. System becomes a 2D grid**

* Partitioning spreads data
* Replication spreads risk

### **5. Partition-aware routing is essential**

* System or client must map keys to partitions

### **6. Challenges arise**

* hot partitions,
* rebalancing,
* cross-partition transactions,
* metadata management.

### **7. The combination is what makes modern distributed databases work**

Partitioning without replication is unsafe.
Replication without partitioning is unscalable.
**Together they form the basis of modern large-scale systems.**