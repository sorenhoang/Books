## **Topic 6.5 — Request Routing**

Request routing is the mechanism that determines **which node in a distributed system should handle a given read or write**. After data is partitioned, the system must know:

* *Which partition contains the data?*
* *Which replica of that partition should serve the request?*
* *What happens during failover or rebalancing?*

Routing is a fundamental part of distributed databases like Cassandra, MongoDB, DynamoDB, CockroachDB, Elasticsearch, HBase, and Bigtable.

This topic explains:

* How routing maps keys → partitions → replicas
* Three major routing architectures
* How systems ensure correctness during failures or rebalancing
* Trade-offs between performance, simplicity, and metadata complexity

---

# **I. Why Request Routing Is Needed**

After partitioning the data, a database must route incoming operations to the correct partition.

Example table partitioning:

| Partition | Key Range / Hash Range | Hosts          |
| --------- | ---------------------- | -------------- |
| P1        | A–F                    | Node 1, Node 2 |
| P2        | G–O                    | Node 3, Node 4 |
| P3        | P–Z                    | Node 5, Node 6 |

When querying:

```
GET("Hello")
```

→ must map `"Hello"` → P2 → Node 3 or Node 4.

This process must be:

* **fast** (microseconds)
* **correct** (always the right node)
* **up-to-date** even during rebalancing or failures

---

# **II. Components of Routing**

A routing system must maintain:

### **1. Partition Map**

Maps key → partition ID
Partition ID → replica nodes

### **2. Routing Policy**

Which replica to choose? (leader / follower / quorum)

### **3. Failure Handling**

Detect node failure and reroute automatically.

### **4. Metadata Distribution**

All nodes (and clients) must share a consistent view of partition layout.

---

# **III. Routing Strategies**

There are **three main routing designs**:

1. **Client-side routing**
2. **Routing tier / query coordinator**
3. **Random node routing (decentralized gossip)**

Each has different operational trade-offs.

---

# **IV. Strategy 1 — Client-Side Routing**

The client has full knowledge of the partition map:

```
Client → looks up partition → selects replica → sends request directly
```

### **Advantages**

✔ Lowest latency (direct path)
✔ Avoids extra network hops
✔ Scales very well (no central bottleneck)
✔ Good for large clusters

### **Disadvantages**

❌ Client must maintain up-to-date metadata
❌ More complex client libraries
❌ Rebalancing or node changes require notifying clients

### **Used in**

* Cassandra
* Riak
* Kafka (clients know topic partition → broker mapping)
* Elasticsearch smart clients

### **Implementation Detail**

Clients pull metadata via:

* gossip protocol,
* metadata servers,
* bootstrap configuration.

When routing changes, clients receive metadata updates.

---

# **V. Strategy 2 — Routing Tier (Query Coordinator)**

A dedicated layer (stateless servers) routes requests.

```
Client → Router → Node
```

### **How it works**

* Clients send requests to a routing service.
* Router uses partition map to forward request to correct replica.
* Router may perform:

  * scatter/gather for global queries,
  * combining results,
  * retrying failed nodes.

### **Advantages**

✔ Simple client logic
✔ Central control of routing
✔ Easy metadata management
✔ Can implement sophisticated query optimization

### **Disadvantages**

❌ Extra network hop (added latency)
❌ Router becomes bottleneck unless scaled horizontally
❌ Router failures require redundancy
❌ Higher operational complexity

### **Used in**

* MongoDB (`mongos`)
* HBase RegionServers + HMaster
* Elasticsearch coordinating nodes
* Vitess VTGate
* SQL engines like Presto, Trino

Routing tier is a very flexible approach for complex distributed queries.

---

# **VI. Strategy 3 — Random Node Routing (Decentralized)**

Clients can contact **any node**; that node forwards or handles request:

```
Client → Any Node → Correct Node
```

Nodes internally route the request among themselves.

### **Advantages**

✔ Simple client
✔ No dedicated routing layer
✔ Good for gossip-based or peer-to-peer systems

### **Disadvantages**

❌ Node handling extra work forwarding requests
❌ Inefficiency for high-traffic workloads
❌ Harder to diagnose routing paths

### **Used in**

* Dynamo-style systems
* Cassandra coordinator node behavior
* Couchbase
* Some peer-to-peer NoSQL systems

This model reduces complexity at the client while maintaining decentralization.

---

# **VII. Maintaining the Partition Map**

Routing relies on having **accurate metadata** describing:

* partitions,
* replica sets,
* leaders,
* partition boundaries,
* node status.

This metadata can be managed by:

### **1. Config Servers (strongly consistent)**

Used in:

* MongoDB config servers
* Elasticsearch master nodes
* Spanner placement metadata

These servers store authoritative cluster configuration.

---

### **2. Gossip Protocol (eventually consistent)**

Used in:

* Cassandra
* Dynamo
* Riak

Nodes broadcast changes (joins, leaves, failures) to others.

Gossip is decentralized but may cause:

* temporary inconsistencies,
* read/write retries.

---

### **3. Distributed Consensus (Raft/Paxos)**

Used in:

* CockroachDB
* Etcd (Kubernetes)
* TiKV / TiDB

Consensus ensures *all nodes agree* on:

* partition distribution,
* leader identity,
* replica state.

These systems support **strongly consistent routing metadata**.

---

# **VIII. Choosing Which Replica to Read From**

Routing involves selecting not just the correct partition but also the ideal replica.

Factors:

### **1. Leader or Follower Reads**

Leader:

* strong consistency
* higher write load

Follower:

* reduced load on leader
* possible stale reads

Systems like PostgreSQL replicas or MongoDB secondaries use follower reads for availability.

---

### **2. Nearest Replica (Geo-Aware Routing)**

In multi-region systems, choose replica closest to user.

Example:

* User in Asia read → Asia partition replica
* User in EU read → EU replica

Used in:

* DynamoDB global tables
* CockroachDB geo-partitioning
* Spanner multi-region instances

---

### **3. Quorum-Based Reads**

Leaderless systems (Cassandra, Dynamo) use quorum routing:

```
Coordinator → queries R replicas → merges results
```

Coordinator selects replicas based on:

* health,
* latency,
* backup replicas.

---

### **4. Load-Based Replica Selection**

Choose replica with:

* lowest CPU load,
* fastest disk I/O,
* lowest queue depth,
* best historical latency.

Elasticsearch uses **adaptive replica selection**.

---

# **IX. Routing During Failures**

Routing must adapt instantly during:

* node crashes,
* network partitions,
* leader changes,
* rebalancing operations.

Key mechanisms:

### **1. Retry on Next Replica**

If a replica fails, the router picks another.

### **2. Metadata Refresh**

Routing map refreshed from:

* gossip,
* config servers,
* consensus groups.

### **3. Leader Election**

If leader fails:

* consensus elects new leader,
* routing routes writes to new leader,
* old leader blocked (fencing).

### **4. Graceful Degradation**

If cluster is partially down:

* quorum systems still operate,
* non-quorum systems may downgrade reads,
* stale reads served when consistent reads unavailable.

---

# **X. Routing During Rebalancing**

Routing complexity increases during:

* partition moves,
* splitting/merging partitions,
* leader relocation.

Routing must ensure:

* **No request goes to wrong partition**,
* **No writes get lost**,
* **No read sees partially-copied data**,
* **Old and new locations both remain safe** until cutover.

Mechanisms used:

### **1. Versioned Routing Maps**

Client/router uses a version number:

```
Using routing map v12
Cluster publishes map v13
Client reloads mapping and switches safely
```

### **2. Dual Writes**

Writes go to both old and new replicas until cutover.

### **3. Fencing Tokens**

Prevent old leaders from accepting writes after relocation.

### **4. Redirects**

Node receiving a request forwards it to the new location.

---

# **XI. Real-World Routing Implementations**

### **Cassandra**

* Client learns topology via gossip
* Any node can act as coordinator
* Coordinator routes to replicas based on replication strategy

### **MongoDB**

* `mongos` router handles routing
* Maintains metadata from config servers

### **Elasticsearch**

* Smart clients or coordinating nodes
* Master node provides shard placement info

### **CockroachDB**

* SQL gateway routes queries
* Metadata via strongly-consistent KV store and Raft

### **DynamoDB**

* Internal service handles routing automatically
* Range-based and hash-based partition mapping

### **Kafka**

* Clients fetch partition → broker mapping from metadata API

Each system uses routing adapted to its consistency and partitioning model.

---

# **XII. Key Takeaways for Topic 6.5**

### **1. Request routing is essential for a partitioned system to function.**

It connects **keys → partitions → replicas**.

### **2. Three routing models:**

* **Client-side routing** → fastest, scalable
* **Routing tier** → simplest for clients, flexible
* **Decentralized routing** → peer-to-peer, fault-tolerant

### **3. Routing must track partition metadata accurately.**

### **4. Routing must adapt instantly to failures.**

### **5. Routing must support rebalancing without downtime.**

### **6. Routing must select the correct replica based on:**

* consistency guarantees,
* latency,
* load,
* geographic locality.

### **7. Metadata consistency determines routing correctness.**

Strong consistency (Spanner/CockroachDB) vs eventual (Cassandra).