## **Topic 6.6 — Summary**

**Chapter 6 — Partitioning** brings together the fundamental concepts required to scale databases horizontally. Partitioning (also called *sharding*) allows a system to handle more data and more traffic than a single machine could ever support. But partitioning introduces new complexity around indexing, routing, balancing, and replication.

This summary distills the core principles, trade-offs, and design patterns from the chapter.

---

# **I. Why Partitioning Exists**

Partitioning is required when:

* a single node cannot store all data,
* a single node cannot handle all read/write load,
* resilience and organizational isolation are desired.

Partitioning provides:

### ✔ Horizontal scalability

By distributing data across many nodes.

### ✔ Parallelism

Each partition is independent and processes its own operations.

### ✔ Fault isolation

A failure affects only the partitions on affected nodes.

Partitioning is foundational to systems like Cassandra, DynamoDB, MongoDB, CockroachDB, Elasticsearch, Bigtable, and more.

---

# **II. Building Blocks of Partitioning**

Partitioning must answer **four key questions**:

1. **How do we divide the data?**
   (range, hash, consistent hashing)

2. **How do we maintain indexes across partitions?**
   (local vs global secondary indexes)

3. **How do we move partitions when nodes change?**
   (rebalancing)

4. **How do we route queries to the right partition?**
   (client-side, routing tier, decentralized routing)

These four problems define the architecture of any sharded database.

---

# **III. Major Partitioning Strategies**

### **1. Range-Based Partitioning**

* Uses sorted ranges of keys.
* Great for range queries and locality.
* Risk of hotspots (e.g., time-based keys).
* Adaptive splitting helps reduce imbalances.

### **2. Hash-Based Partitioning**

* Hash(key) → partition.
* Great load distribution.
* Breaks ordering and range queries.
* Rebalancing expensive unless combined with consistent hashing.

### **3. Consistent Hashing**

* Node additions/removals move minimal data.
* Works well for eventually consistent systems.
* Requires virtual nodes for smooth distribution.

These strategies balance simplicity, performance, and operational overhead.

---

# **IV. Partitioning + Replication**

Replication works **per partition**, meaning:

* Each partition has its **own replication group**.
* Leader election, journaling, consensus, or quorum logic runs independently.
* Failures and rebalancing are isolated.

This allows massive clusters because coordination is **localized** to the partition, not global.

Replication provides:

* durability (multiple copies),
* availability (serve reads/writes even with failures),
* performance scaling (read load split across replicas).

---

# **V. Indexing in a Partitioned System**

Indexes become more complex once data is partitioned.

### **Local Secondary Indexes**

* Stored next to the data.
* Simple writes, but reads may need scatter/gather across partitions.
* Cannot enforce global uniqueness.

### **Global Secondary Indexes**

* Stored in separate partitions.
* Enable efficient querying across the cluster.
* Writes become multi-partition operations.
* Require atomic commit or tolerate eventual consistency.

Indexing trade-offs influence consistency, performance, and operational complexity.

---

# **VI. Rebalancing Partitions**

Rebalancing redistributes data when:

* adding nodes,
* removing nodes,
* skew occurs,
* replication factor changes.

Key requirements:

* **minimal data movement**,
* **online operation**,
* **metadata correctness**,
* **preserving availability**,
* **no lost writes**.

Techniques:

* consistent hashing minimization of movement,
* range splitting and merging,
* snapshot + redo-log replay,
* throttled movement to avoid overload.

Rebalancing is one of the hardest engineering challenges in distributed systems.

---

# **VII. Request Routing**

Routing determines:

* which partition stores a key,
* which replica to read or write to,
* how to handle failover or rebalancing transitions.

### **Routing Models**

1. **Client-side routing**

   * Fastest and most scalable
   * Clients maintain metadata

2. **Routing tier (query coordinator)**

   * Simplifies client logic
   * Allows query planning
   * Adds an extra hop

3. **Decentralized routing (any node can forward)**

   * Peer-to-peer flexibility
   * Common in Dynamo-style systems

Routing must stay correct during:

* node failures,
* leader changes,
* partition moves,
* replica additions/removals.

Metadata distribution (gossip, config servers, consensus) is crucial.

---

# **VIII. Consistency and Availability Implications**

Partitioning interacts deeply with consistency models:

* Writes touching a single partition can be strongly consistent.
* Cross-partition transactions require:

  * 2PC,
  * timestamp ordering,
  * or application-level compensation.

Eventual consistency is easier to achieve than strong consistency in partitioned setups.

Replication + partitioning trade-offs must be understood together.

---

# **IX. Operational Observations**

### **1. Small partitions improve balance and flexibility**

Systems prefer many small partitions ("ranges", "shards", or "regions").

### **2. Hotspot detection and mitigation is essential**

Skew cannot always be predicted.

### **3. Metadata consistency determines correctness**

Stale metadata → wrong routing → errors or stale reads.

### **4. Rebalancing must be throttled**

To avoid destabilizing the cluster.

### **5. Controlled routing during rebalancing is crucial**

Ensures correctness and prevents double-writes or missing writes.

---

# **X. Real-World Patterns from Modern Systems**

### **Cassandra**

* Consistent hashing + virtual nodes
* Client-side routing
* Leaderless replication

### **MongoDB**

* Sharded clusters
* Routing via mongos
* Range or hashed sharding

### **Bigtable / HBase**

* Range partitions (“regions”)
* Automatic splitting
* Master assigns regions

### **CockroachDB**

* Raft groups per range
* Strongly consistent metadata
* Dynamic splitting and load balancing

### **DynamoDB**

* Massive internal partition layer
* Adaptive capacity to avoid hotspots
* Fully managed routing and rebalancing

These systems prove that partitioning is not optional at scale—it defines the system’s architecture.

---

# **XI. Final Key Takeaways**

### **1. Partitioning allows databases to scale horizontally.**

Without partitioning, system growth hits a hard limit.

### **2. A partitioning strategy must balance load distribution and query patterns.**

No "one-size fits all".

### **3. Secondary indexes across partitions are difficult.**

Choose between simplicity (local) and efficiency (global).

### **4. Rebalancing is central to maintaining long-term cluster health.**

### **5. Routing must be accurate, fast, and adaptive to failures.**

### **6. Partitioning requires tight integration with replication.**

Every partition needs its own replication group for safety.

### **7. The complexity of partitioning moves from hardware → software.**

Modern distributed databases solve extremely complex problems under the hood to present a simple API externally.
