## **Topic 6.4 — Rebalancing Partitions**

Rebalancing is the process of **redistributing partitions across nodes** in a distributed database when:

* nodes are **added** (scale out),
* nodes are **removed** (scale in or hardware failure),
* load becomes **uneven** (hotspots),
* storage becomes **imbalanced**,
* or maintenance requires moving data.

Rebalancing is essential to keep the cluster **healthy, scalable, and performant**, but it is also one of the **most complex and delicate operations** in distributed systems. It must be done:

* **online** (no downtime),
* **incrementally** (avoid massive network load),
* **consistently** (no lost or duplicated data),
* **efficiently** (minimize movement).

This topic explains how modern systems rebalance partitions safely and automatically.

---

# **I. Why Rebalancing Is Needed**

### **1. Uneven Data Distribution (Skew)**

Some partitions grow much larger than others:

* uneven key distribution,
* hot keys,
* time-based writes,
* user IDs clustered.

Large partitions slow down:

* compactions,
* reads/writes,
* recovery time.

### **2. Uneven Load Distribution**

One partition may receive far more:

* reads (popular user),
* writes (timestamp trend),
* updates (hot product ID).

This becomes a **hotspot** that limits throughput.

### **3. Adding Nodes**

A cluster that grows from 4 → 10 nodes requires redistributing partitions; otherwise new nodes sit idle.

### **4. Removing Nodes**

When a node fails or is decommissioned, its partitions must be reallocated to other nodes to preserve replication factor.

### **5. Changing Replication Factor**

Increasing RF from 2 → 3 also forces repartitioning or data copying.

Rebalancing ensures the system remains **efficient and fault-tolerant** as topology changes.

---

# **II. Requirements for Good Rebalancing**

A rebalancing mechanism must meet the following requirements:

### ✔ **Minimal Data Movement**

The most expensive operation in a distributed DB is **moving data**.
Good designs limit movement to the **minimum necessary**.

### ✔ **Online Operation**

Reads and writes must continue normally while rebalancing occurs.

### ✔ **Consistency**

No dropped keys, no duplicates, no split-brain.

### ✔ **Automatic & Adaptive**

No manual intervention; system detects imbalance and acts.

### ✔ **Preserve Replication Guarantees**

Every partition must maintain:

* correct replication factor (RF),
* correct leader/follower roles.

### ✔ **Throttle the Data Movement**

To avoid overwhelming:

* network bandwidth,
* disk I/O,
* user-facing latency.

---

# **III. Rebalancing Techniques by Partitioning Strategy**

Rebalancing depends heavily on how partitions are assigned.

---

## **1. Hash Partitioning with Modulo**

For simple partitioning:

```
partition = hash(key) % N
```

When **N changes**, almost *all* keys get remapped.
This causes:

❌ **massive data movement**
❌ catastrophic rebalancing cost
❌ downtime or long maintenance windows

→ Not used in modern large-scale systems.

---

## **2. Consistent Hashing**

Consistent hashing minimizes movement:

* Keys and nodes are placed on a **hash ring**.
* Only keys between old & new node ranges move.

When adding one node:

```
Only ~1/N of the keys move.
```

When removing a node:

```
Only keys in its segment move.
```

But for good load balancing, consistent hashing needs:

* many **virtual nodes** (vnodes),
* careful ring management.

Used in:

* Cassandra,
* Dynamo,
* Riak,
* Akamai CDN.

---

## **3. Range-Based Partitioning with Splitting**

Range-based systems (HBase, Bigtable, CockroachDB) rebalance by:

### **Splitting**

If a partition (range) becomes too large or too hot:

```
Range [A–Z] → split into [A–M], [M–Z]
```

### **Moving**

The split partitions are moved to under-utilized nodes.

### **Merging**

If partitions become too small or skew stabilizes:

```
[A–D], [D–G] → merge into [A–G]
```

This enables very fine-grained, adaptive balancing.

---

# **IV. How Rebalancing Works Internally**

Rebalancing involves multiple coordinated steps:

---

## **1. Identify imbalance**

Metrics:

* partition size,
* read/write throughput,
* CPU,
* disk usage,
* latency.

Triggers:

* threshold crossing,
* periodic background scanning.

---

## **2. Choose donor and receiver nodes**

Examples:

* node with too much data → donor
* node with too little → receiver

Algorithms:

* greedy balancing,
* load scoring,
* round-robin placement,
* holistic placement (consider replicas).

---

## **3. Copy data to new node**

Copying is done in stages:

### **Stage 1 — Snapshot copy**

Copy the entire partition snapshot.

### **Stage 2 — Incremental catch-up**

Replay changes (WAL or logs) that occurred during snapshot.

### **Stage 3 — Final sync + switch**

Once caught up, switch traffic over.

Rebalancing uses the **same mechanics** as adding a new follower.

---

## **4. Update metadata / routing information**

Once the new partition is fully replicated:

* metadata service updates key → partition → location mapping,
* routing layers direct new traffic to the new node,
* old node gradually stops serving that partition.

Metadata systems:

* Cassandra: token metadata ring
* MongoDB: config servers
* CockroachDB: gossip + Raft metadata tables
* Spanner: placement metadata in TrueTime-synchronized tablets

---

# **V. Rebalancing Challenges**

### **1. Hotspots**

Even after rebalancing, hot keys may overload a partition.

Solutions:

* key salting,
* dynamic range splitting,
* spreading writes across partitions.

---

### **2. Operational Safety**

Rebalancing can overload the cluster:

* heavy disk I/O during transfers,
* network saturation,
* interference with compactions,
* latency spikes.

Systems must **throttle** rebalancing speed.

---

### **3. Maintaining Consistency**

During rebalancing:

* reads must hit correct and up-to-date data,
* writes must not be lost.

Approaches:

* dual writes during transfer,
* versioned routing,
* cutover with barrier/fencing tokens,
* read repair correctness guarantees.

---

### **4. Leader/Follower Distribution**

After rebalancing, you may end up with:

* too many leaders on one node,
* too many followers on another.

Systems must:

* redistribute leadership,
* ensure even CPU load,
* ensure even write responsibility.

CockroachDB automates leader rebalancing using Raft metrics.

---

# **VI. Examples in Real Distributed Databases**

### **Cassandra**

* Uses consistent hashing + virtual nodes.
* Rebalancing is lightweight.
* Node bootstrap and decommission handle data movement.

### **MongoDB Sharded**

* Range or hashed sharding.
* Balancer daemon moves chunks between shards.
* Uses two-phase commit for metadata updates.

### **HBase / Bigtable**

* Regions split automatically.
* Master assigns regions to region servers.
* Load-aware placement algorithm.

### **Elasticsearch**

* Resilient shard allocation.
* Routing nodes and master nodes coordinate shard movement.
* Auto-recovery + rebalance after failure.

### **CockroachDB**

* Range splits with Raft consensus per range.
* Automatic replica rebalancing.
* Multi-dimensional metrics used for placement.

### **Kafka**

* Partition rebalancing across brokers.
* Supports partition reassignment with minimal downtime.
* Throttled movement to preserve performance.

---

# **VII. Rebalancing Costs**

Rebalancing is expensive:

* **Network bandwidth** is consumed heavily during copying.
* Disk I/O is used for snapshots and logs.
* CPU load increases for compaction and encoding.
* Latency for user requests may spike temporarily.

This is why systems often:

* perform rebalancing slowly,
* schedule it during low-traffic hours,
* or keep it always-on but throttled.

---

# **VIII. Best Practices for Rebalancing**

### ✔ Start with many small partitions

Allows flexible and fine-grained balancing.

### ✔ Use virtual nodes

Avoids large moves and node hotspots.

### ✔ Throttle replication/copy speed

Protects user-visible latency.

### ✔ Avoid hash(mod N) partitioning

Catastrophic on reshard.

### ✔ Monitor skew continuously

Detect hot partitions early.

### ✔ Make rebalancing automatic

Manual rebalancing does NOT scale.

---

# **IX. Key Takeaways for Topic 6.4**

### **1. Rebalancing ensures even distribution of data and load.**

Necessary when adding/removing nodes or when hotspots appear.

### **2. Must be online and safe.**

No downtime, no lost writes, consistent view during movement.

### **3. Must minimize data movement.**

Consistent hashing and fine-grained range partitioning achieve this.

### **4. Uses snapshot + log replay mechanics.**

Same mechanism as adding new follower replicas.

### **5. Must preserve replication factor.**

Leaders and followers both need balancing.

### **6. Requires coordination with routing metadata.**

Routing tables or partition maps must update atomically.

### **7. One of the hardest parts of distributed database operation.**

Even well-designed systems struggle with efficient rebalancing under extreme loads.
