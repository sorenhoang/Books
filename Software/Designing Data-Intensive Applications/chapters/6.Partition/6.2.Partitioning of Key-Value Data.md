## **Topic 6.2 — Partitioning of Key-Value Data**

Partitioning of key-value data is the foundational technique for distributing data across multiple nodes in a scalable and fault-tolerant way. Since key-value workloads are common in modern databases (NoSQL, log stores, cache systems, search engines), an efficient strategy for mapping keys to partitions is essential.

This topic explains how databases decide **which node stores which key**, how to avoid **hotspots**, how to maintain **balance**, and how to support **scaling up/down** safely.

---

# **I. Why Key-Value Partitioning Matters**

A simple key-value API:

```
put(key, value)
get(key)
delete(key)
```

When data becomes too large for one machine, we must partition it:

* **Key-based partitioning** determines which node stores which key.
* Partitioning must distribute load evenly.
* Partitioning must minimize movement during rebalancing.
* Partitioning must route requests efficiently.

Goals of good partitioning:

1. Even distribution of keys across nodes
2. Even distribution of read/write traffic
3. Minimize hotspots
4. Support adding/removing nodes easily
5. Simple and fast routing of queries

Partitioning is fundamental for **horizontal scalability**.

---

# **II. Common Partitioning Strategies**

There are three primary strategies:

1. **Partitioning by Key Range**
2. **Partitioning by Hash of Key**
3. **Consistent Hashing**

Each comes with trade-offs around locality, load balance, and rebalancing complexity.

---

# **III. Partitioning by Key Range**

Keys are ordered, and ranges of keys are assigned to each partition.

Example:

```
Partition 1 → keys < "h"
Partition 2 → "h" ≤ keys < "p"
Partition 3 → keys ≥ "p"
```

### **Advantages**

✔ Supports efficient range queries (`key >= "a" AND key < "f"`).
✔ Naturally preserves ordering.
✔ Ideal for sorted indexes, time-series keys, and B-tree-based storage.

### **Disadvantages**

❌ Risk of **hotspots** if key distribution is skewed.

Example:

* If keys are timestamps, newest timestamp always goes to the same partition → one partition overloaded.

❌ Requires rebalancing if distribution becomes uneven
(e.g., shifting range boundaries).

### **Used in**

* HBase
* Bigtable
* MongoDB (range sharding)
* TiDB
* CockroachDB (range-based partitioning)

Range-based partitioning is excellent for **ordered queries**, but needs careful handling to avoid hotspots.

---

# **IV. Partitioning by Hash of Key**

Use a hash function (MD5, SHA-1, MurmurHash) to map each key to a partition ID.

Example:

```
partition = hash(key) % num_partitions
```

### **Advantages**

✔ Excellent load balancing
✔ Prevents hotspots from sequential keys
✔ Very simple and fast computation

### **Disadvantages**

❌ Loses key ordering → range queries must be broadcasted
❌ Rebalancing expensive when number of partitions changes

Changing `num_partitions` requires remapping most keys, causing massive data movement.

### **Used in**

* Redis Cluster
* Cassandra (originally with consistent hashing variant)
* DynamoDB internal architecture

Hash partitioning is ideal for **uniform distribution**, especially in pure key-value operations.

---

# **V. Consistent Hashing**

Consistent hashing is a smarter hashing strategy originally from Dynamo and used widely in distributed databases and caching systems.

### **Key idea**

We map both **nodes** and **keys** onto a virtual **hash ring**:

```
0 → 2^32 range forms a ring
Nodes placed at various points
Key assigned to first node clockwise
```

### **Advantages**

✔ Minimal movement when adding/removing nodes → only nearby keys move
✔ Natural fault tolerance
✔ Good load balance (especially with virtual nodes)

### **Disadvantages**

❌ Not perfect load balancing unless using many virtual replicas
❌ Still loses key ordering
❌ Metadata structures required to maintain ring

### **Used in**

* Amazon Dynamo
* Cassandra
* Riak
* Akamai CDN
* Envoy proxy load balancing
* Many distributed cache systems (Memcached clusters)

Consistent hashing is excellent for **scaling elastic clusters**, especially when nodes join/leave frequently.

---

# **VI. Avoiding Hotspots**

Hotspots occur when one partition receives disproportionately more load. Example scenarios:

* Sequential primary keys (auto-increment IDs)
* Timestamp-based keys (newest always go to one partition)
* Popular keys (celebrity account)

### **Strategies to reduce hotspots:**

#### **1. Hash Partitioning**

Randomizes key distribution.

#### **2. Salting**

Add a prefix to the key:

Instead of:

```
key = "user:123"
```

Use:

```
key = "3:user:123"   (where 3 = hash(key) % 10)
```

This spreads writes across multiple partitions.

#### **3. Pre-split ranges**

Divide ranges more finely early.

#### **4. Adaptive splitting**

Databases like Bigtable auto-split hot ranges.

#### **5. Move “hot” records into separate partitions**

Used in sharded SQL databases.

Hotspot mitigation is a major operational concern in partitioning.

---

# **VII. Static vs Dynamic Partitioning**

### **Static Partitioning**

* Fixed number of partitions.
* Changing number of partitions requires painful resharding.

Used by:

* early MySQL sharding setups,
* Redis cluster (fixed 16384 slots).

### **Dynamic Partitioning**

* Partitions split or merge automatically.
* System balances load without downtime.

Used by:

* Bigtable / HBase (split large regions),
* CockroachDB (range splits),
* Kafka (topic partition expansion).

Dynamic partitioning improves usability and long-term scalability.

---

# **VIII. Partition Size and Quantity**

Trade-offs:

### **Fewer large partitions**

* easier to manage
* fewer network hops
* but rebalancing large partitions is slow
* single partition can become a hotspot

### **Many small partitions**

* fine-grained rebalancing
* better parallelism
* higher metadata overhead
* more consensus groups (in partitioned Raft systems)

Modern systems prefer **many small partitions** (e.g., 64MB–256MB ranges).

---

# **IX. Routing Queries to Partitions**

Once partitioning is decided, the system must route requests correctly.

Strategies:

* **Client-side routing** (smart client)
* **Routing tier** (MongoDB mongos, Elasticsearch coordinator)
* **Coordinator node** (Cassandra coordinator)
* **Metadata lookup** from config servers

Routing ensures queries hit the correct partition and avoid broadcast.

Routing is covered more deeply in Topic 6.5.

---

# **X. Partition Rebalancing**

Adding or removing nodes triggers:

* migrating partitions,
* updating metadata,
* redistributing leaders,
* minimizing disruption.

Key goals:

* automated rebalancing,
* minimal data movement,
* no downtime,
* avoid overloading network/disk.

This is covered in depth in Topic 6.4.

---

# **XI. How Partitioning Interacts With Replication**

Every partition has multiple replicas.

Example:

```
Partition 34
  Replica1 (Leader) — Node A
  Replica2 (Follower) — Node B
  Replica3 (Follower) — Node C
```

Each replica stores **the same subset of keys**, which differ from other partitions.

Key points:

* Writes are routed to leader (or quorum replicas)
* Reads may go to any replica
* Replication occurs **within** each partition
* Failover happens **per partition**, not cluster-wide

This avoids global coordination, enabling **massive horizontal scale**.

---

# **XII. Real-World Examples**

### **Cassandra**

* Partition key hashed with Murmur3.
* Token ring controls partition ownership.
* Per-partition replication with quorum.

### **MongoDB Sharding**

* Range or hashed sharding.
* Config servers track shard map.

### **DynamoDB**

* Adaptive capacity to auto-balance hot keys.
* Partitions grow with traffic and size.

### **HBase / Bigtable**

* Range-based regions.
* Automatic splits + region server migration.

### **Elasticsearch / OpenSearch**

* Index shards.
* Routing based on document ID hash.
* Automatic shard rebalancing.

These systems rely on **robust and adaptive partitioning** for scalability.

---

# **XIII. Key Takeaways for Topic 6.2**

### **1. Partitioning of key-value data is essential for horizontal scaling.**

### **2. Three main techniques:**

* **Range partitioning** → good for ordered queries, risk hotspots
* **Hash partitioning** → good load balance, no ordering
* **Consistent hashing** → minimal data movement when nodes change

### **3. Partitioning must avoid hotspots**

Sequential keys can overload partitions if naive.

### **4. Dynamic partitioning is critical for long-term health**

Auto-splitting, auto-balancing.

### **5. Routing must be efficient**

System or client needs a fast mapping from key → partition.

### **6. Partitioning works hand-in-hand with replication**

Each partition has its own replication group.

Partitioning decides **where** your data goes.
Replication decides **how many copies** of it exist.
Together they form the backbone of scalable distributed databases.