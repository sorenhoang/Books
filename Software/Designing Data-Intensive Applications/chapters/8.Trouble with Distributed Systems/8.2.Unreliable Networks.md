## **Topic 8.2 — Unreliable Networks**

In distributed systems, **the network is the most unreliable component**. Even if all nodes are healthy, the network between them can delay, drop, duplicate, or reorder messages. Most distributed system failures that look like “node failures” are actually **network failures in disguise**.

This topic explains **why networks are unreliable**, **what kinds of failures occur**, and **how these failures shape system design**.

---

# **I. The Network Is Not Reliable by Default**

A common (and dangerous) assumption is:

> “If a message didn’t arrive, the receiver must be down.”

In reality:

* the message may be delayed,
* dropped,
* duplicated,
* reordered,
* or stuck behind congestion.

From the sender’s perspective, **silence is ambiguous**.

Distributed systems must treat the network as:

* asynchronous,
* unpredictable,
* and unreliable.

---

# **II. Types of Network Faults**

---

## **1. Packet Loss**

Messages may be dropped due to:

* network congestion,
* faulty hardware,
* overloaded switches,
* transient link failures.

Packet loss may be:

* rare but non-zero,
* bursty (many lost in a short time),
* asymmetric (one direction only).

Loss causes:

* timeouts,
* retries,
* duplicate messages.

---

## **2. Packet Delay (Latency Spikes)**

Even if packets are not lost, they may be **delayed**:

* queueing in routers,
* GC pauses at sender or receiver,
* CPU starvation,
* virtualization overhead.

Latency is:

* variable,
* heavy-tailed (rare but extreme spikes),
* not bounded in asynchronous systems.

A delayed message is indistinguishable from a lost message.

---

## **3. Packet Reordering**

Packets may arrive **out of order** due to:

* multiple network paths,
* load balancing,
* retransmissions.

If the application assumes ordering, this can cause:

* stale updates overwriting new ones,
* protocol violations.

Ordering must be enforced at the application or protocol layer.

---

## **4. Packet Duplication**

Networks may deliver the **same message more than once**:

* retries at lower layers,
* timeout-based retransmission,
* sender uncertainty.

Therefore:

> Distributed systems must assume **at-least-once delivery**.

Exactly-once delivery is extremely difficult and expensive.

---

## **5. Network Partitions**

A **network partition** occurs when:

* nodes are alive,
* but cannot communicate with some other nodes.

Examples:

* split data centers,
* misconfigured routing rules,
* firewall issues,
* fiber cuts.

During a partition:

* each side may believe the other is down,
* both sides may continue operating independently.

This can lead to:

* split-brain,
* data divergence,
* conflicting decisions.

---

# **III. Network Faults vs Node Faults**

Key insight:

> **You cannot reliably distinguish a slow node from a dead node using the network.**

No response could mean:

* node crashed,
* node overloaded,
* network partition,
* packet delay.

This ambiguity is fundamental and unavoidable.

---

# **IV. Why TCP Does Not “Fix” This**

TCP provides:

* reliable byte stream,
* retransmission,
* congestion control,
* in-order delivery.

But TCP does **not** guarantee:

* bounded latency,
* detection of peer failure,
* timely delivery,
* message-level semantics.

If a TCP connection hangs:

* the OS may retry for minutes,
* the application may block indefinitely.

Applications must still implement:

* timeouts,
* retries,
* cancellation.

---

# **V. Timeouts: A Necessary Evil**

Since failure detection is impossible, systems rely on **timeouts**.

### **Timeout trade-offs**

* **Short timeout**:

  * faster failure detection,
  * more false positives,
  * unnecessary retries or failovers.

* **Long timeout**:

  * fewer false positives,
  * slow recovery,
  * poor user experience.

There is **no perfect timeout** value.

Timeouts must be:

* workload-dependent,
* continuously tuned,
* adaptive when possible.

---

# **VI. Retries and Their Dangers**

Retries are necessary but dangerous.

### **Problems caused by retries**

* duplicate requests,
* duplicated writes,
* amplified load (retry storms),
* cascading failures.

Example:

* a slow service triggers retries,
* retries increase load,
* service slows further,
* more retries → meltdown.

---

## **Safe Retry Design**

Retries must be:

* **idempotent**, or
* **deduplicated** using request IDs.

Best practices:

* exponential backoff,
* jitter (randomized delays),
* retry budgets,
* circuit breakers.

---

# **VII. Message Delivery Guarantees**

Distributed systems typically choose between:

### **At-most-once**

* message delivered zero or one time
* no duplicates
* may lose messages

### **At-least-once**

* message delivered one or more times
* no loss
* duplicates possible

### **Exactly-once**

* message delivered exactly once
* extremely difficult and expensive
* usually implemented as *effectively-once* semantics

Most systems use **at-least-once delivery** and handle duplicates at the application layer.

---

# **VIII. Network Partitions and the CAP Theorem**

During a network partition, systems must choose:

* **Consistency (C)**: all nodes see the same data
* **Availability (A)**: system continues to accept requests

Partition tolerance (P) is mandatory in distributed systems.

Thus:

> **You cannot have both C and A during a partition.**

This is not a design choice but a **physical reality** imposed by unreliable networks.

---

# **IX. Split-Brain and Its Prevention**

A classic failure scenario:

* network partition splits cluster into two halves,
* both sides elect a leader,
* both accept writes.

This is **split-brain**.

Prevented by:

* quorum-based decisions,
* majority voting,
* fencing tokens,
* consensus protocols (Raft, Paxos).

Only the side with quorum may proceed.

---

# **X. Designing for Unreliable Networks**

Distributed systems must be designed with the assumption that the network will fail.

---

## **1. Use Idempotent Operations**

Ensure repeating an operation does not change the result.

Example:

* use request IDs,
* conditional updates,
* compare-and-set.

---

## **2. Avoid Synchronous Waiting on Many Nodes**

Waiting for all nodes increases the chance of failure.

Prefer:

* quorums,
* partial responses,
* async processing.

---

## **3. Prefer Asynchronous Communication**

Async decouples sender and receiver and improves resilience.

Used in:

* message queues,
* event streams,
* background jobs.

---

## **4. Implement Backpressure**

Prevent overload from retries and slow consumers.

---

## **5. Use Circuit Breakers**

Stop sending requests to unhealthy nodes temporarily.

---

## **6. Assume Messages Can Be Lost, Delayed, or Duplicated**

Design protocols accordingly.

---

# **XI. Example: Distributed Database Write**

Client writes to leader:

* request delayed → client retries
* leader receives request twice
* without deduplication → double write

Therefore:

* writes must be idempotent,
* or include unique transaction IDs.

---

# **XII. Real-World Network Failure Examples**

* AWS us-east-1 outages caused by network misconfiguration
* Google BGP incidents causing partial reachability
* Kubernetes cluster failures due to CNI plugin bugs
* Microservice meltdowns due to retry storms

Most large outages are caused by **network issues**, not machine crashes.

---

# **XIII. Key Takeaways for Topic 8.2**

### **1. Networks are unreliable, asynchronous, and unpredictable.**

### **2. Message loss, delay, duplication, and reordering are normal.**

### **3. No response ≠ node failure.**

### **4. Timeouts and retries are mandatory but dangerous.**

### **5. Exactly-once delivery is impractical at scale.**

### **6. Network partitions force consistency vs availability trade-offs.**

### **7. Systems must be designed assuming the network will fail.**

---

### **One-Sentence Summary**

> **In distributed systems, the network cannot be trusted to deliver messages reliably or on time, so systems must be designed to tolerate loss, delay, duplication, and partitions without assuming perfect communication.**