## **Topic 8.1 — Faults and Partial Failures**

One of the core reasons distributed systems are difficult is that **failure is no longer binary**. In a single-machine system, a failure usually means the machine is either **working or completely down**. In a distributed system, components can fail **independently and partially**, creating ambiguous, hard-to-reason-about situations.

This topic explains **what partial failures are**, **why they are unavoidable**, and **how they fundamentally shape the design of distributed systems**.

---

# **I. What Is a Partial Failure?**

A **partial failure** occurs when **some parts of a system fail while others continue to operate**, and the system as a whole keeps running.

Examples:

* One server crashes, others stay up
* A network link drops packets intermittently
* One replica is slow but not dead
* A disk returns corrupted data occasionally
* A service responds but with long delays

In these situations:

* The system is neither fully correct nor fully failed
* Different nodes may have **different views of reality**

This is the defining characteristic of distributed systems.

---

# **II. Why Partial Failures Do Not Exist in Single-Machine Systems**

On a single machine:

* CPU, memory, disk, and OS fail together
* If the machine crashes → everything stops
* If it’s alive → everything works (mostly)

Failure detection is simple:

* Process alive → system works
* Process dead → system failed

In distributed systems:

* Components fail independently
* The system continues operating in degraded mode
* Failure detection is uncertain

This difference is **the root of most distributed systems complexity**.

---

# **III. Types of Faults in Distributed Systems**

Distributed systems face many kinds of faults, often simultaneously.

---

## **1. Process Failures**

A process may:

* crash,
* be killed,
* run out of memory,
* hang indefinitely,
* pause due to GC or CPU starvation.

From another node’s perspective:

* no response,
* delayed response,
* intermittent response.

It is impossible to know whether the process is:

* dead,
* slow,
* overloaded,
* or just temporarily paused.

---

## **2. Network Failures**

Networks are unreliable by nature:

* packets can be lost,
* packets can be duplicated,
* packets can be delayed,
* packets can arrive out of order,
* network partitions can isolate nodes.

Network failures are especially dangerous because:

* they look exactly like node failures,
* they can affect only some connections,
* they can heal without notice.

---

## **3. Hardware Faults**

Examples:

* disk sector corruption,
* NIC malfunction,
* memory bit flips,
* power supply instability.

Hardware faults may:

* corrupt data silently,
* slow down components,
* cause transient failures.

These faults are **more common than most people expect** at scale.

---

## **4. Software Faults**

Bugs can cause:

* memory leaks,
* deadlocks,
* livelocks,
* race conditions,
* crashes under rare conditions.

A key danger:

> Software bugs often affect **all replicas running the same code**, creating correlated failures.

---

## **5. Human Faults**

Humans are a major source of failures:

* misconfiguration,
* accidental deletion,
* incorrect deployments,
* wrong firewall rules,
* expired certificates.

Many large outages are caused not by hardware but by **operator error**.

---

# **IV. Why Partial Failures Are So Hard to Handle**

### **1. No Clear Failure Signal**

In distributed systems:

* no response ≠ node is dead
* slow response ≠ failure
* network partition ≠ crash

The system must guess using **timeouts**, which are heuristic.

---

### **2. Different Nodes See Different Realities**

Node A may see:

* Node B unreachable

Node C may still communicate with B normally.

There is **no single source of truth** about system state.

---

### **3. Failures Can Cascade**

One slow node can:

* cause retries,
* increase load on others,
* trigger timeouts,
* cause more retries,
* overload the entire system.

This is known as a **failure cascade**.

---

### **4. Recovery Is Ambiguous**

If a node disappears and then returns:

* Was it down?
* Did it miss updates?
* Should it resume its old role?
* Is its data still valid?

Improper recovery can cause:

* split brain,
* data loss,
* stale leaders.

---

# **V. The Fallacy of Perfect Failure Detection**

There is **no reliable way** to distinguish:

* a crashed node,
* a slow node,
* a partitioned node.

This is formalized by the **FLP impossibility result**:

> In an asynchronous system with failures, it is impossible to guarantee consensus with perfect accuracy.

Therefore:

* failure detection is always **probabilistic**
* timeouts are guesses, not proofs

---

# **VI. Design Principles Derived from Partial Failures**

Distributed systems must be designed assuming partial failures are normal.

---

## **1. Timeouts Are Mandatory**

Every remote call must have:

* a timeout,
* retry logic,
* backoff strategy.

Blocking forever is unacceptable.

---

## **2. Retries Must Be Safe**

Retries can cause:

* duplicate requests,
* duplicate writes,
* inconsistent state.

Thus operations must be:

* idempotent,
* deduplicated,
* protected by unique request IDs.

---

## **3. Redundancy Is Required**

To survive partial failures:

* replicate data,
* replicate services,
* deploy across failure domains.

But redundancy increases coordination complexity.

---

## **4. Graceful Degradation**

Systems should:

* reduce functionality under stress,
* shed load,
* return partial results,
* prioritize critical operations.

Failing *softly* is better than total outage.

---

## **5. Fencing and Epochs**

When roles change (e.g., leader election), systems must prevent old nodes from acting.

This is done using:

* fencing tokens,
* epochs,
* generation numbers.

Prevents “zombie” nodes from corrupting state.

---

# **VII. Partial Failures and Consistency**

Partial failures force trade-offs:

* waiting for all nodes → low availability
* proceeding with partial information → weaker consistency

This leads to:

* CAP theorem trade-offs,
* quorum-based systems,
* eventual consistency models.

Consistency is not free—it’s paid for with availability or latency.

---

# **VIII. Example: Database Replica Failure**

Imagine:

* Leader A
* Followers B, C

If A becomes unreachable:

* Is A dead?
* Or is the network partitioned?
* Should B become leader?

If A later returns:

* Should it resume leadership?
* Or step down?

Without careful design:

* two leaders may exist,
* writes may diverge,
* data may be lost.

This is why **consensus protocols** exist.

---

# **IX. Example: Timeout Misconfiguration**

If timeouts are:

* too short → false positives → unnecessary failovers
* too long → slow failure detection → long outages

There is **no perfect timeout value**.

Timeout tuning is a continuous operational challenge.

---

# **X. What Partial Failures Force Us to Accept**

In distributed systems:

* failure is normal, not exceptional
* components lie (by omission or delay)
* perfect knowledge is impossible
* uncertainty is fundamental

Thus systems must:

* tolerate uncertainty,
* make progress without full knowledge,
* recover from wrong assumptions.

---

# **XI. Key Takeaways for Topic 8.1**

### **1. Partial failures are the defining challenge of distributed systems.**

### **2. Nodes and networks fail independently and unpredictably.**

### **3. Failure detection is never perfect—only probabilistic.**

### **4. Timeouts, retries, and redundancy are essential but dangerous if misused.**

### **5. Systems must tolerate uncertainty and conflicting views of reality.**

### **6. Many higher-level mechanisms (consensus, fencing, quorums) exist because of partial failures.**

---

### **One-Sentence Summary**

> **Distributed systems fail partially, not completely, and this fundamental uncertainty forces designers to trade simplicity for robustness, consistency for availability, and certainty for progress.**
