# **Topic 1.3 — Scalability**

Scalability refers to a system’s ability to handle increased load without sacrificing performance, reliability, or efficiency. A scalable system maintains acceptable response time and throughput as demand grows. The goal is not just to run the system today, but to ensure it can support future traffic growth, data expansion, and evolving use cases.

A system is considered scalable if, when the workload increases, operators can expand the system’s capacity proportionally—either by adding more resources or improving architecture—without needing to rewrite the system fundamentally. Scalability is not a single feature but a property of system design, architecture choices, and deployment strategy.

---

### **Understanding Load**

Scaling begins with understanding the nature of load. Load describes how users and applications interact with the system, and different systems experience different types of load. Key dimensions include:

* **Request Rate:** Number of requests per second (RPS / QPS) for APIs or services.
* **Data Volume:** Amount of stored or processed data.
* **Concurrency:** Number of simultaneous active users or processes.
* **Read/Write Ratio:** Whether the workload is read-heavy, write-heavy, or balanced.
* **Access Patterns:** Sequential reads vs random access, bursts vs steady-state load.

Correctly identifying load patterns helps determine whether the system needs caching, replication, sharding, batching, or new processing models.

---

### **Performance Metrics: Latency and Throughput**

Two core metrics define scalability:

* **Latency** measures how long it takes to process a request.
* **Throughput** measures how many requests the system can process per unit of time.

A scalable system maintains low latency as throughput capacity increases. Measuring only average latency is misleading; high percentiles (p95, p99) matter more because they identify worst-case user experience. For example, an average response time of 10ms with p99 of 500ms may signal bottlenecks or uneven performance.

---

### **Elasticity vs Fixed Scaling**

Systems may scale in two ways:

* **Elastic Scaling:** Automatically adjusting based on load (e.g., Kubernetes autoscaling). Suitable for variable or bursty workloads.
* **Manual or Planned Scaling:** Increasing capacity through scheduled upgrades, often seen in legacy systems or predictable enterprise environments.

Elasticity reduces operational overhead but introduces new complexity in monitoring, capacity planning, and cost control.

---

### **Scaling Strategies**

There are two fundamental approaches:

1. **Vertical Scaling (Scale Up):**

   * Upgrade a single machine: more CPU, RAM, storage.
   * Simple and effective up to hardware limits.
   * Limited by cost and diminishing returns.

2. **Horizontal Scaling (Scale Out):**

   * Add more machines or replicas to distribute load.
   * Enables near unlimited capacity growth.
   * Requires system design for distribution: load balancing, replication, and partitioning.

Most web-scale systems rely heavily on horizontal scaling because it aligns with cloud infrastructure patterns and failure isolation.

---

### **Managing Read and Write Load**

Different workloads require different scaling techniques:

* **Read Scaling:**

  * Achieved through replication, caching, and indexing.
  * Read replicas offload primary databases.
  * Caches (Redis, CDN) reduce pressure on core systems.

* **Write Scaling:**

  * More difficult because write operations change state.
  * Requires advanced techniques like sharding/partitioning, batching, eventual consistency, and queueing.

Systems like Cassandra, DynamoDB, and Kafka were designed specifically to scale writes efficiently.

---

### **Partitioning and Data Distribution**

Partitioning (sharding) distributes data across multiple nodes so the dataset and load grow beyond a single machine's limits. Choosing a good partitioning strategy ensures:

* Balanced load distribution
* Minimal cross-node coordination
* Efficient read/write routing

However, poor partitioning can create hotspots, uneven load, and operational challenges like rebalancing and migrations.

---

### **Caching as a Scalability Layer**

Caching improves performance by storing frequently accessed data in faster systems. It helps reduce database load, optimize latency, and absorb spikes. However, caching also introduces complexity like stale data, invalidation strategies, and cache consistency problems.

---

### **Scaling Analytics vs Transactional Workloads**

Transactional systems (OLTP) handle individual requests that modify data, requiring consistency and low latency. Analytical systems (OLAP) process large volumes of historical data for insights, often prioritizing throughput over latency. As systems grow, separating OLTP and OLAP through techniques like **ETL, CDC**, or **streaming pipelines** becomes necessary.

---

### **Workload Growth and Architectural Evolution**

Scalability decisions evolve with product maturity. Many systems follow a pattern:

1. Single server
2. Add read replicas
3. Add caching layer
4. Apply sharding
5. Introduce message queues and streaming
6. Move to distributed, multi-region architecture

Growth forces architectural change, and scalability planning prevents crisis-driven rewrites.

---

### **Scalability Trade-offs**

Every scalability technique introduces trade-offs:

* **Consistency vs Performance**
* **Complexity vs Cost**
* **Latency vs Durability**
* **Predictability vs Resource Utilization**

No single approach fits all systems; scalability is always contextual.

---

### **Key Principle**

A scalable system is one that can accommodate growth—whether in users, data, or operations—by expanding resources and architecture without significant degradation or redesign. Scalability is about anticipating future needs and designing systems capable of evolving smoothly under increasing demand.