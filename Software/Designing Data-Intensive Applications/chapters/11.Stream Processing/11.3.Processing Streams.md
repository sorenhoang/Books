# **Processing Streams**

Stream processing involves taking one or more input streams and producing one or more output streams (derived data). Unlike batch jobs which have a fixed start and end, stream jobs operate on **unbounded datasets** that never end. A piece of code that processes streams in this manner is known as an **operator** or a **job**.

### **Uses of Stream Processing**

There are several distinct patterns for utilizing stream processing:

* **Complex Event Processing (CEP):** Geared toward searching for specific event patterns (e.g., fraud detection or financial trading rules). CEP systems store queries and flow data through them, which is the reverse of standard databases where data is stored and queries are transient.
* **Stream Analytics:** Focuses on aggregations and statistical metrics over time, such as calculating the rate of events or rolling averages. These systems often employ probabilistic algorithms (like Bloom filters or HyperLogLog) to manage memory usage.
* **Maintaining Materialized Views:** Stream processors can consume a log of changes (via Change Data Capture) to keep derived systems, such as search indexes, caches, and data warehouses, in sync with a source database.
* **Search on Streams:** Similar to CEP, this involves registering complex search queries (e.g., media monitoring for specific keywords) and matching individual documents against them as they pass through the stream.

### **Reasoning About Time**

A critical challenge in stream processing is distinguishing between **event time** (when an event actually occurred) and **processing time** (when the stream processor observed the event).

* **Processing Lag:** Relying on the processing machine's system clock breaks down when there are processing delays, network faults, or restarts. For example, if a processor is restarting, a backlog of events may be processed quickly, creating an artificial spike in "requests per second" if measured by processing time rather than event time.
* **Stragglers:** When defining time windows based on event time, it is difficult to know when all events for a specific window have arrived. Systems must decide whether to ignore **straggler events** or publish corrections to previously emitted values.
* **Window Types:**
  * **Tumbling Window:** Fixed length, non-overlapping (e.g., every minute).
  * **Hopping Window:** Fixed length, allowing overlap (e.g., a 5-minute window calculated every 1 minute).
  * **Sliding Window:** Covers all events that occur within a specific interval of each other.
  * **Session Window:** Grouped by user activity; the window ends when a user has been inactive for a specific duration.

### **Stream Joins**

Joining data in streams is more complex than in batch jobs because new events can appear at any time. There are three primary types of stream joins:

1. **Stream-Stream Join (Window Join):** Joins two streams of events that occur within a specific time window. For example, joining a "search" event with a "click" event to calculate click-through rates.
2. **Stream-Table Join (Stream Enrichment):** Joins a stream of activity events with a database (e.g., joining user clicks with user profile data). To avoid slow remote queries, the processor often loads a local copy of the database, maintained via a changelog stream.
3. **Table-Table Join (Materialized View Maintenance):** Joins two changelogs (e.g., tweets and follows) to maintain a materialized view (e.g., a user's timeline). This corresponds to a database join being updated whenever the underlying tables change.

### **Fault Tolerance**

Because streams are infinite, you cannot simply wait for a job to finish before making output visible. Stream processors must handle faults while preventing data loss or double-processing, often aiming for **exactly-once semantics** (more accurately called **effectively-once**).

* **Microbatching:** Used by Spark Streaming, this breaks the stream into small blocks and treats each as a miniature batch process.
* **Checkpointing:** Used by Apache Flink, this periodically generates rolling checkpoints of state to durable storage. If a crash occurs, the operator restarts from the last checkpoint.
* **Idempotence:** To ensure safety when outputs cannot be rolled back (like sending an email), operations can be made idempotent. For example, writing to a database can include the offset of the message that triggered the write, preventing the same update from being applied twice during a retry.
